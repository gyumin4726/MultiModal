import torch
import pandas as pd
from PIL import Image
import torchvision.transforms as transforms
from tqdm import tqdm
import re
from config import seed_everything
import os
from datetime import datetime

# ëª¨ë“ˆ ì„í¬íŠ¸ (ì•ˆì „í•œ ë°©ì‹ìœ¼ë¡œ)
try:
    from vision_encoder import load_vision_encoder
    VISION_AVAILABLE = True
except ImportError as e:
    print(f"Warning: Vision encoder not available - {e}")
    VISION_AVAILABLE = False

try:
    from text_encoder import load_text_encoder
    TEXT_AVAILABLE = True
except ImportError as e:
    print(f"Warning: Text encoder not available - {e}")
    TEXT_AVAILABLE = False

try:
    from language_model import load_language_model
    LLM_AVAILABLE = True
except ImportError as e:
    print(f"Warning: Language model not available - {e}")
    LLM_AVAILABLE = False

try:
    from multimodal_fusion import MultiModalFusion
    FUSION_AVAILABLE = True
except ImportError as e:
    print(f"Warning: Multimodal fusion not available - {e}")
    FUSION_AVAILABLE = False

def extract_answer_letter(text):
    """LLM ì‘ë‹µì—ì„œ A, B, C, D ë‹µë³€ ì¶”ì¶œ (ê°œì„ ëœ ë²„ì „)"""
    if not text:
        print(f"âš ï¸ Empty response, defaulting to A")
        return 'A'
    
    text = text.strip().upper()
    
    # ë‹¤ì–‘í•œ íŒ¨í„´ ê²€ìƒ‰ (ìš°ì„ ìˆœìœ„ ìˆœ)
    patterns = [
        r'\b([ABCD])\b(?:\s*[.:]|\s*$)',  # ë‹¨ë… ë¬¸ì + ë§ˆì¹¨í‘œ/ì½œë¡ /ë
        r'ANSWER[:\s]*([ABCD])',           # "ANSWER: A" í˜•íƒœ
        r'SOLUTION[:\s]*([ABCD])',         # "SOLUTION: A" í˜•íƒœ  
        r'([ABCD])[.:]',                   # "A." ë˜ëŠ” "A:" í˜•íƒœ
        r'\b([ABCD])\b',                   # ë‹¨ìˆœ ë¬¸ì ë§¤ì¹­ (ë§ˆì§€ë§‰ ìˆ˜ë‹¨)
    ]
    
    for pattern in patterns:
        matches = re.findall(pattern, text)
        if matches:
            answer = matches[-1]  # ë§ˆì§€ë§‰ ë§¤ì¹˜ ì‚¬ìš©
            print(f"âœ… Extracted answer '{answer}' using pattern: {pattern}")
            return answer
    
    # ëª¨ë“  íŒ¨í„´ ì‹¤íŒ¨ì‹œ ë””ë²„ê¹… ì •ë³´ ì¶œë ¥
    print(f"âŒ No answer pattern found in: '{text[:100]}...'")
    print(f"âš ï¸ Defaulting to A")
    return 'A'

def count_parameters(model):
    """ëª¨ë¸ì˜ ì´ íŒŒë¼ë¯¸í„° ê°œìˆ˜ ê³„ì‚°"""
    if model is None:
        return 0
    return sum(p.numel() for p in model.parameters())

def format_parameter_count(count):
    """íŒŒë¼ë¯¸í„° ê°œìˆ˜ë¥¼ ì½ê¸° ì‰¬ìš´ í˜•íƒœë¡œ ë³€í™˜"""
    if count >= 1e9:
        return f"{count/1e9:.2f}B"
    elif count >= 1e6:
        return f"{count/1e6:.2f}M"
    elif count >= 1e3:
        return f"{count/1e3:.2f}K"
    else:
        return str(count)

def load_models():
    """ëª¨ë“  ëª¨ë¸ ë¡œë”© (ì•ˆì „í•œ ë°©ì‹)"""
    print("ğŸš€ Loading MultiModal VQA System...")
    
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    print(f"âœ… Using device: {device}")
    
    models = {
        'vision_encoder': None,
        'text_encoder': None,
        'language_model': None,
        'multimodal_fusion': None,
        'device': device
    }
    
    total_params = 0
    
    # Vision Encoder (ì„ íƒì )
    if VISION_AVAILABLE:
        print("ğŸ–¼ï¸ Loading Vision Encoder...")
        try:
            vision_encoder = load_vision_encoder(
                model_name='vmamba_tiny_s1l8',
                pretrained_path='./vssm1_tiny_0230s_ckpt_epoch_264.pth',
                output_dim=768,
                frozen_stages=1
            ).to(device)
            models['vision_encoder'] = vision_encoder
            vision_params = count_parameters(vision_encoder)
            total_params += vision_params
            print(f"âœ… Vision Encoder loaded! Parameters: {format_parameter_count(vision_params)}")
        except Exception as e:
            print(f"Warning: VMamba not available - {e}")
            models['vision_encoder'] = None
    
    # Text Encoder (ì„ íƒì )
    if TEXT_AVAILABLE:
        print("ğŸ“ Loading Text Encoder...")
        try:
            text_encoder = load_text_encoder(
                model_type='default',
                output_dim=768,
                device=device
            )
            models['text_encoder'] = text_encoder
            text_params = count_parameters(text_encoder)
            total_params += text_params
            print(f"âœ… Text Encoder loaded! Parameters: {format_parameter_count(text_params)}")
        except Exception as e:
            print(f"Warning: Text Encoder failed - {e}")
            models['text_encoder'] = None
    
    # Language Model (í•„ìˆ˜)
    if LLM_AVAILABLE:
        print("ğŸ¤– Loading Language Model...")
        try:
            # ë¹ ë¥¸ í…ŒìŠ¤íŠ¸ë¥¼ ìœ„í•œ ëª¨ë¸ ì„ íƒ (í™˜ê²½ë³€ìˆ˜ë¡œ ì œì–´ ê°€ëŠ¥)
            import os
            model_choice = os.getenv('LLM_MODEL', 'microsoft/phi-2')  # ê¸°ë³¸ê°’: phi-2
            
            language_model = load_language_model(
                model_name=model_choice,
                device=device
            )
            models['language_model'] = language_model
            llm_params = count_parameters(language_model)
            total_params += llm_params
            print(f"âœ… Language Model loaded! Parameters: {format_parameter_count(llm_params)}")
        except Exception as e:
            print(f"Warning: Language Model failed - {e}")
            print("    Falling back to text-only mode...")
            models['language_model'] = None
    
    # MultiModal Fusion (ì„ íƒì )
    if FUSION_AVAILABLE and models['vision_encoder'] is not None and models['text_encoder'] is not None:
        print("ğŸ”— Loading MultiModal Fusion...")
        try:
            multimodal_fusion = MultiModalFusion(
                vision_dim=768,
                text_dim=768,
                hidden_dim=512,
                output_dim=768
            ).to(device)
            models['multimodal_fusion'] = multimodal_fusion
            fusion_params = count_parameters(multimodal_fusion)
            total_params += fusion_params
            print(f"âœ… MultiModal Fusion loaded! Parameters: {format_parameter_count(fusion_params)}")
        except Exception as e:
            print(f"Warning: MultiModal Fusion failed - {e}")
            models['multimodal_fusion'] = None
    
    # Image Transform (í•­ìƒ í•„ìš”)
    models['transform'] = transforms.Compose([
        transforms.Resize((224, 224)),
        transforms.ToTensor(),
        transforms.Normalize(mean=[0.485, 0.456, 0.406],
                           std=[0.229, 0.224, 0.225])
    ])
    
    # ë¡œë”©ëœ ëª¨ë¸ ìš”ì•½
    print("\nğŸ“‹ Model Loading Summary:")
    print(f"   Vision Encoder: {'âœ…' if models['vision_encoder'] else 'âŒ'}")
    print(f"   Text Encoder: {'âœ…' if models['text_encoder'] else 'âŒ'}")
    print(f"   Language Model: {'âœ…' if models['language_model'] else 'âŒ'}")
    print(f"   Multimodal Fusion: {'âœ…' if models['multimodal_fusion'] else 'âŒ'}")
    
    # ì´ íŒŒë¼ë¯¸í„° ê°œìˆ˜ ì¶œë ¥
    print("\nğŸ”¢ Total Parameter Count:")
    print(f"   Combined Models: {format_parameter_count(total_params)} parameters")
    print(f"   Exact Count: {total_params:,} parameters")
    
    return models

def process_vqa_sample(models, image_path, question, choices, sample_idx):
    """ë‹¨ì¼ VQA ìƒ˜í”Œ ì²˜ë¦¬ (ì•ˆì „í•œ ë°©ì‹)"""
    try:
        # ì´ë¯¸ì§€ ê²½ë¡œ í™•ì¸
        if not os.path.exists(image_path):
            print(f"Warning: Image not found: {image_path}")
            return 'A'
        
        # Vision Features ì¶”ì¶œ (ì„ íƒì )
        vision_features = None
        if models['vision_encoder'] is not None:
            with torch.no_grad():
                try:
                    vision_features = models['vision_encoder'](image_path)
                except Exception as e:
                    print(f"Warning: Vision encoding failed - {e}")
        
        # í”„ë¡¬í”„íŠ¸ëŠ” ì´ì œ LLM ë‚´ë¶€ì—ì„œ í”¼ì²˜ì™€ í•¨ê»˜ êµ¬ì„±ë¨
        
        # Text Features ì¶”ì¶œ (ì„ íƒì )
        text_features = None
        if models['text_encoder'] is not None:
            with torch.no_grad():
                try:
                    text_features = models['text_encoder'](question)
                except Exception as e:
                    print(f"Warning: Text encoding failed - {e}")
        
        # MultiModal Fusion (ì„ íƒì )
        fused_features = None
        if (models['multimodal_fusion'] is not None and 
            vision_features is not None and 
            text_features is not None):
            with torch.no_grad():
                try:
                    fused_features = models['multimodal_fusion'](vision_features, text_features)
                except Exception as e:
                    print(f"Warning: Multimodal fusion failed - {e}")
        
        # í”¼ì²˜ ì¶”ì¶œ ìƒíƒœ í™•ì¸ (ë””ë²„ê¹…ìš©)
        features_available = []
        if vision_features is not None:
            features_available.append(f"Vision({vision_features.shape})")
        if text_features is not None:
            features_available.append(f"Text({text_features.shape})")
        if fused_features is not None:
            features_available.append(f"Fused({fused_features.shape})")
        
        # LLM ì¶”ë¡  (í”¼ì²˜ í™œìš©)
        if models['language_model'] is not None:
            try:
                # í”¼ì²˜ ê¸°ë°˜ ì¶”ë¡  ì‹œë„
                if vision_features is not None or text_features is not None or fused_features is not None:
                    print(f"ğŸš€ Using features: {', '.join(features_available)}")
                    response = models['language_model'].answer_question_with_features(
                        question=question,
                        choices=choices,
                        vision_features=vision_features,
                        text_features=text_features,
                        fused_features=fused_features
                    )
                    answer = extract_answer_letter(response)
                    # ë””ë²„ê¹…: ì²˜ìŒ 3ê°œ ìƒ˜í”Œì˜ ì‘ë‹µ ì¶œë ¥
                    if sample_idx < 3:
                        print(f"ğŸ” Sample {sample_idx+1} LLM Response: '{response}'")
                        print(f"ğŸ” Response length: {len(response)}")
                        print(f"ğŸ¯ Extracted Answer: {answer}")
                        print("-" * 50)
                else:
                    print("âš ï¸ No features available, using text-only mode")
                    # í”¼ì²˜ê°€ ì—†ìœ¼ë©´ ê¸°ë³¸ ë°©ì‹
                    response = models['language_model'].answer_question_simple(question, choices)
                    answer = extract_answer_letter(response)
            except Exception as e:
                print(f"Warning: LLM generation failed - {e}")
                answer = 'A'
        else:
            # LLMì´ ì—†ìœ¼ë©´ ë‹¨ìˆœ íœ´ë¦¬ìŠ¤í‹±
            answer = 'A'  # ê¸°ë³¸ê°’
            
        return answer
        
    except Exception as e:
        print(f"âŒ Error processing sample: {e}")
        return 'A'

def main():
    """ë©”ì¸ ì¶”ë¡  í•¨ìˆ˜"""
    # ì‹œë“œ ê³ ì •
    seed_everything()
    
    print("ğŸš€ Starting MultiModal VQA Inference...")
    print("="*60)
    
    # ëª¨ë¸ ë¡œë”©
    models = load_models()
    
    # ìµœì†Œí•œ í•˜ë‚˜ì˜ ëª¨ë¸ì´ë¼ë„ ë¡œë”©ë˜ì—ˆëŠ”ì§€ í™•ì¸
    if all(v is None for k, v in models.items() if k not in ['transform', 'device']):
        print("âŒ No models loaded successfully. Exiting...")
        return
    
    # ë°ì´í„° ë¡œë”©
    print("\nğŸ“Š Loading test data...")
    if not os.path.exists('./dev_test.csv'):
        print("âŒ dev_test.csv not found!")
        return
        
    test = pd.read_csv('./dev_test.csv')
    print(f"ğŸ“‹ Total samples: {len(test)}")
    
    # ì¶”ë¡ 
    print("\nğŸ” Starting inference...")
    results = []
    
    for idx, (_, row) in enumerate(tqdm(test.iterrows(), total=len(test), desc="Processing")):
        image_path = row['img_path']
        question = row['Question']
        choices = [row[c] for c in ['A', 'B', 'C', 'D']]
        
        # VQA ì²˜ë¦¬
        answer = process_vqa_sample(models, image_path, question, choices, sample_idx=idx)
        results.append(answer)
    
    print('âœ… Inference completed!')
    
    # ê²°ê³¼ ì €ì¥
    print("\nğŸ“ Saving results...")
    
    # í˜„ì¬ ë‚ ì§œì™€ ì‹œê°„ìœ¼ë¡œ íŒŒì¼ëª… ìƒì„±
    current_time = datetime.now().strftime("%Y%m%d_%H%M%S")
    output_filename = f'baseline_submit_{current_time}.csv'
    
    if os.path.exists('./sample_submission.csv'):
        submission = pd.read_csv('./sample_submission.csv')
        submission['answer'] = results
        submission.to_csv(f'./{output_filename}', index=False)
        print(f"âœ… Results saved to '{output_filename}'")
    else:
        # sample_submission.csvê°€ ì—†ìœ¼ë©´ ì§ì ‘ ìƒì„±
        submission = pd.DataFrame({
            'ID': range(len(results)),
            'answer': results
        })
        submission.to_csv(f'./{output_filename}', index=False)
        print(f"âœ… Results saved to '{output_filename}' (created new format)")
    
    # ê²°ê³¼ ë¶„ì„
    print(f"\nğŸ“Š Answer distribution:")
    for answer in ['A', 'B', 'C', 'D']:
        count = results.count(answer)
        print(f"   {answer}: {count} ({count/len(results)*100:.1f}%)")
    
    print("\nğŸ‰ VQA inference completed successfully!")

if __name__ == "__main__":
    main() 