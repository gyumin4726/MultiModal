import torch
import pandas as pd
from PIL import Image
import torchvision.transforms as transforms
from tqdm import tqdm
import re
from config import seed_everything
from vision_encoder import load_vision_encoder
from text_encoder import load_text_encoder
from language_model import load_language_model
from multimodal_fusion import MultiModalFusion
import os

def extract_answer_letter(text):
    """ì‘ë‹µì—ì„œ A, B, C, D ì¶”ì¶œ"""
    text = text.strip().upper()
    
    # íŒ¨í„´ ë§¤ì¹­ìœ¼ë¡œ ë‹µë³€ ì¶”ì¶œ
    patterns = [
        r'\b([ABCD])\b',  # ë‹¨ë… ë¬¸ì
        r'ë‹µ:\s*([ABCD])',  # ë‹µ: A í˜•íƒœ
        r'ì •ë‹µ:\s*([ABCD])',  # ì •ë‹µ: A í˜•íƒœ
        r'Answer:\s*([ABCD])',  # Answer: A í˜•íƒœ
    ]
    
    for pattern in patterns:
        match = re.search(pattern, text)
        if match:
            return match.group(1)
    
    # ì²« ë²ˆì§¸ë¡œ ë‚˜íƒ€ë‚˜ëŠ” A, B, C, D ë°˜í™˜
    for char in text:
        if char in 'ABCD':
            return char
            
    return 'A'  # ê¸°ë³¸ê°’

def load_models():
    """ëª¨ë“  ëª¨ë¸ ë¡œë”©"""
    print("ğŸš€ Loading MultiModal VQA System...")
    
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    print(f"ğŸ”§ Using device: {device}")
    
    # Vision Encoder
    print("ğŸ–¼ï¸ Loading Vision Encoder...")
    try:
        vision_encoder = load_vision_encoder(
            model_name='vmamba_tiny_s1l8',
            pretrained_path='./vssm1_tiny_0230s_ckpt_epoch_264.pth',
            output_dim=768,
            frozen_stages=1
        ).to(device)
        print("âœ… Vision Encoder loaded!")
    except Exception as e:
        print(f"âŒ Vision Encoder failed: {e}")
        vision_encoder = None
    
    # Text Encoder
    print("ğŸ“ Loading Text Encoder...")
    try:
        text_encoder = load_text_encoder(
            model_type='default',
            output_dim=768
        )
        print("âœ… Text Encoder loaded!")
    except Exception as e:
        print(f"âŒ Text Encoder failed: {e}")
        text_encoder = None
    
    # Language Model
    print("ğŸ¤– Loading Language Model...")
    try:
        language_model = load_language_model(model_name="microsoft/phi-2")
        print("âœ… Language Model loaded!")
    except Exception as e:
        print(f"âŒ Language Model failed: {e}")
        language_model = None
    
    # MultiModal Fusion
    print("ğŸ”— Loading MultiModal Fusion...")
    try:
        multimodal_fusion = MultiModalFusion(
            vision_dim=768,
            text_dim=768,
            hidden_dim=512,
            num_heads=8
        ).to(device)
        print("âœ… MultiModal Fusion loaded!")
    except Exception as e:
        print(f"âŒ MultiModal Fusion failed: {e}")
        multimodal_fusion = None
    
    # Image Transform
    transform = transforms.Compose([
        transforms.Resize((224, 224)),
        transforms.ToTensor(),
        transforms.Normalize(mean=[0.485, 0.456, 0.406],
                           std=[0.229, 0.224, 0.225])
    ])
    
    return {
        'vision_encoder': vision_encoder,
        'text_encoder': text_encoder,
        'language_model': language_model,
        'multimodal_fusion': multimodal_fusion,
        'transform': transform,
        'device': device
    }

def process_vqa_sample(models, image_path, question, choices):
    """ë‹¨ì¼ VQA ìƒ˜í”Œ ì²˜ë¦¬"""
    try:
        # ì´ë¯¸ì§€ ë¡œë”©
        if not os.path.exists(image_path):
            return 'A'
            
        image = Image.open(image_path).convert("RGB")
        image_tensor = models['transform'](image).unsqueeze(0).to(models['device'])
        
        # Vision Features ì¶”ì¶œ
        with torch.no_grad():
            if models['vision_encoder'] is not None:
                vision_features = models['vision_encoder'](image_tensor)
            else:
                vision_features = None
        
        # í”„ë¡¬í”„íŠ¸ êµ¬ì„±
        choices_text = "\n".join([f"{chr(65+i)}. {choice}" for i, choice in enumerate(choices)])
        
        prompt = (
            "You are a helpful AI that answers multiple-choice questions based on the given image.\n"
            "Select the best answer from A, B, C, or D.\n\n"
            f"Question: {question}\n"
            f"{choices_text}\n"
            "Answer:"
        )
        
        # Text Features ì¶”ì¶œ (ì˜µì…˜)
        if models['text_encoder'] is not None:
            with torch.no_grad():
                text_features = models['text_encoder']([question])
        else:
            text_features = None
        
        # MultiModal Fusion (ì˜µì…˜)
        if (models['multimodal_fusion'] is not None and 
            vision_features is not None and 
            text_features is not None):
            with torch.no_grad():
                fused_features = models['multimodal_fusion'](vision_features, text_features)
        
        # LLM ì¶”ë¡ 
        if models['language_model'] is not None:
            response = models['language_model'].generate_text(
                prompt,
                max_new_tokens=5,
                temperature=0.0
            )
            answer = extract_answer_letter(response)
        else:
            answer = 'A'  # ê¸°ë³¸ê°’
            
        return answer
        
    except Exception as e:
        print(f"âŒ Error processing sample: {e}")
        return 'A'

def main():
    """ë©”ì¸ ì¶”ë¡  í•¨ìˆ˜"""
    # ì‹œë“œ ê³ ì •
    seed_everything()
    
    print("ğŸš€ Starting MultiModal VQA Inference...")
    print("="*60)
    
    # ëª¨ë¸ ë¡œë”©
    models = load_models()
    
    # ë°ì´í„° ë¡œë”©
    print("\nğŸ“Š Loading test data...")
    test = pd.read_csv('./dev_test.csv')
    print(f"ğŸ“‹ Total samples: {len(test)}")
    
    # ì¶”ë¡ 
    print("\nğŸ” Starting inference...")
    results = []
    
    for _, row in tqdm(test.iterrows(), total=len(test), desc="Processing"):
        image_path = row['img_path']
        question = row['Question']
        choices = [row[c] for c in ['A', 'B', 'C', 'D']]
        
        # VQA ì²˜ë¦¬
        answer = process_vqa_sample(models, image_path, question, choices)
        results.append(answer)
    
    print('âœ… Inference completed!')
    
    # ê²°ê³¼ ì €ì¥
    print("\nğŸ“ Saving results...")
    submission = pd.read_csv('./sample_submission.csv')
    submission['answer'] = results
    submission.to_csv('./baseline_submit.csv', index=False)
    
    print("âœ… Results saved to 'baseline_submit.csv'")
    
    # ê²°ê³¼ ë¶„ì„
    print(f"\nğŸ“Š Answer distribution:")
    for answer in ['A', 'B', 'C', 'D']:
        count = results.count(answer)
        print(f"   {answer}: {count} ({count/len(results)*100:.1f}%)")
    
    print("\nğŸ‰ VQA inference completed successfully!")

if __name__ == "__main__":
    main() 