import torch
import pandas as pd
from PIL import Image
import torchvision.transforms as transforms
from tqdm import tqdm
import re
from config import seed_everything
import os
from datetime import datetime

# ëª¨ë“ˆ ì„í¬íŠ¸ (BLIP2 ìŠ¤íƒ€ì¼ - ë‹¨ìˆœí™”)
from model.vision_encoder import load_vision_encoder
from model.language_model import load_language_model
# text_encoder, multimodal_fusionì€ ë” ì´ìƒ ì‚¬ìš©í•˜ì§€ ì•ŠìŒ

def extract_answer_letter(text):
    """ê°œì„ ëœ ë‹µë³€ ì¶”ì¶œ - ìƒˆë¡œìš´ í”„ë¡¬í”„íŠ¸ í˜•ì‹ì— ë§ê²Œ"""
    if not text:
        return 'B'
    
    text = text.strip().upper()
    
    # 1. "A." í˜•ì‹ ìš°ì„  ê²€ìƒ‰ (í”„ë¡¬í”„íŠ¸ì—ì„œ ìš”ì²­í•œ í˜•ì‹)
    match = re.search(r'\b([ABCD])\.\s*$', text)
    if match:
        answer = match.group(1)
        print(f"âœ… Found exact format: {answer}.")
        return answer
    
    # 2. "A." í˜•ì‹ (ëì´ ì•„ë‹Œ ê³³ì—ì„œë„)
    match = re.search(r'\b([ABCD])\.', text)
    if match:
        answer = match.group(1)
        print(f"âœ… Found letter with period: {answer}.")
        return answer
    
    # 3. ë‹¨ìˆœíˆ A, B, C, Dë§Œ ìˆëŠ” ê²½ìš°
    match = re.search(r'\b([ABCD])\b', text)
    if match:
        answer = match.group(1)
        print(f"âœ… Found single letter: {answer}")
        return answer
    
    # 4. í…ìŠ¤íŠ¸ ì‹œì‘ ë¶€ë¶„ì˜ ë¬¸ì
    if len(text) > 0 and text[0] in 'ABCD':
        answer = text[0]
        print(f"âœ… Found at start: {answer}")
        return answer
    
    # 5. ì‹¤íŒ¨ì‹œ ê¸°ë³¸ê°’
    print(f"âš ï¸ No clear answer found in: '{text}'")
    return 'B'

def count_parameters(model):
    """ëª¨ë¸ì˜ ì´ íŒŒë¼ë¯¸í„° ê°œìˆ˜ ê³„ì‚°"""
    if model is None:
        return 0
    return sum(p.numel() for p in model.parameters())

def format_parameter_count(count):
    """íŒŒë¼ë¯¸í„° ê°œìˆ˜ë¥¼ ì½ê¸° ì‰¬ìš´ í˜•íƒœë¡œ ë³€í™˜"""
    if count >= 1e9:
        return f"{count/1e9:.2f}B"
    elif count >= 1e6:
        return f"{count/1e6:.2f}M"
    elif count >= 1e3:
        return f"{count/1e3:.2f}K"
    else:
        return str(count)

def validate_image_path(image_path):
    """ì´ë¯¸ì§€ ê²½ë¡œ ê²€ì¦ ë° ìˆ˜ì •"""
    if not os.path.exists(image_path):
        print(f"âš ï¸ Image not found: {image_path}")
        return None
    
    try:
        # ì´ë¯¸ì§€ ë¡œë”© í…ŒìŠ¤íŠ¸
        with Image.open(image_path) as img:
            img = img.convert('RGB')
            print(f"âœ… Image validation OK: {image_path} ({img.size})")
        return image_path
    except Exception as e:
        print(f"âŒ Image loading error: {e}")
        return None

def validate_data_format(row):
    """ë°ì´í„° í˜•ì‹ ê²€ì¦"""
    required_fields = ['Question', 'A', 'B', 'C', 'D']
    for field in required_fields:
        if field not in row or pd.isna(row[field]) or str(row[field]).strip() == '':
            print(f"âŒ Missing or empty field: {field}")
            return False
    return True

def load_models():
    """BLIP2 ìŠ¤íƒ€ì¼ ëª¨ë¸ ë¡œë”©"""
    print("ğŸš€ Loading BLIP2-style VQA System...")
    
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    print(f"âœ… Using device: {device}")
    
    models = {
        'vision_encoder': None,
        'language_model': None,
        'device': device
    }
    
    total_params = 0
    
    # Vision Encoder (Q-Former)
    print("ğŸ–¼ï¸ Loading Vision Encoder with Q-Former...")
    vision_encoder = load_vision_encoder(
        model_name='vit_base_patch16_224',  # ë©”ëª¨ë¦¬ íš¨ìœ¨ì„±ì„ ìœ„í•´ Base ì‚¬ìš©
        pretrained=True,
        output_dim=768,
        frozen_stages=2,  # ë” ë§ì€ ë ˆì´ì–´ freeze (ë¹ ë¥¸ ì¶”ë¡ )
        num_query_tokens=32  # BLIP2 í‘œì¤€
    ).to(device)
    models['vision_encoder'] = vision_encoder
    vision_params = count_parameters(vision_encoder)
    total_params += vision_params
    print(f"âœ… Vision Encoder loaded! Parameters: {format_parameter_count(vision_params)}")
    
    # Language Model (OPT)
    print("ğŸ¤– Loading Multimodal Language Model (OPT)...")
    language_model = load_language_model(
        model_name='facebook/opt-2.7b',  # BLIP2ì—ì„œ ì‚¬ìš©í•˜ëŠ” OPT
        device=device
    )
    models['language_model'] = language_model
    llm_params = count_parameters(language_model)
    total_params += llm_params
    print(f"âœ… Language Model loaded! Parameters: {format_parameter_count(llm_params)}")
    
    # ì´ íŒŒë¼ë¯¸í„° ê°œìˆ˜ ì¶œë ¥
    print("\nğŸ”¢ BLIP2-style System Parameter Count:")
    print(f"   Vision Encoder (Q-Former): {format_parameter_count(vision_params)} parameters")
    print(f"   Language Model (OPT): {format_parameter_count(llm_params)} parameters")
    print(f"   Total System: {format_parameter_count(total_params)} parameters")
    print(f"\nğŸ“¸ Vision Processing: Q-Former with 32 learnable query tokens")
    print(f"ğŸ”— Multimodal Strategy: Direct vision token injection into LLM (BLIP2 style)")
    
    return models

def process_vqa_sample(models, image_path, question, choices, sample_idx):
    """BLIP2 ìŠ¤íƒ€ì¼ VQA ì²˜ë¦¬"""
    
    print(f"ğŸ” Processing Question {sample_idx + 1:02d}/60...")
    
    # ì´ë¯¸ì§€ ê²½ë¡œ ê²€ì¦
    valid_image_path = validate_image_path(image_path)
    if valid_image_path is None:
        print(f"âŒ Skipping due to invalid image: {image_path}")
        return 'B'  # ê¸°ë³¸ê°’ ë°˜í™˜
    
    # ë°ì´í„° ê²€ì¦
    if len(choices) != 4:
        print(f"âŒ Invalid choices count: {len(choices)}")
        return 'B'
    
    try:
        # 1. Q-Formerë¡œ 32ê°œ ë¹„ì „ í† í° ìƒì„±
        with torch.no_grad():
            vision_tokens, attention_weights = models['vision_encoder'].forward_qformer(valid_image_path)
            print(f"âœ… Vision tokens generated: {vision_tokens.shape}")
    
        # 2. ë©€í‹°ëª¨ë‹¬ LLMì´ ë¹„ì „ í† í°ì„ ì§ì ‘ ì²˜ë¦¬
        response = models['language_model'].answer_vqa_multimodal(
            vision_tokens=vision_tokens,
            question=question,
            choices=choices
        )
        answer = extract_answer_letter(response)
        
        # ì²˜ìŒ 3ê°œ ìƒ˜í”Œë§Œ ìƒì„¸ ì¶œë ¥
        if sample_idx < 3:
            print(f"ğŸ“ Q{sample_idx+1}: {question}")
            print(f"ğŸ” Choices: {choices}")
            print(f"ğŸ¤– LLM Response: '{response}'")
            print(f"ğŸ¯ Answer: {answer}")
        else:
            print(f"âœ… Q{sample_idx+1:02d} â†’ {answer}")
        
        return answer
        
    except Exception as e:
        print(f"âŒ Error processing sample {sample_idx+1}: {e}")
        return 'B'  # ì—ëŸ¬ ì‹œ ê¸°ë³¸ê°’

def main():
    """ë©”ì¸ ì¶”ë¡  í•¨ìˆ˜"""
    # ì‹œë“œ ê³ ì •
    seed_everything()
    
    print("ğŸš€ Starting BLIP2-style VQA Inference...")
    print("="*60)
    
    # ëª¨ë¸ ë¡œë”©
    models = load_models()
    
    # ë°ì´í„° ë¡œë”© ë° ê²€ì¦
    print("\nğŸ“Š Loading and validating test data...")
    
    if not os.path.exists('./data/dev_test.csv'):
        print("âŒ Test data file not found: ./data/dev_test.csv")
        return
    
    test = pd.read_csv('./data/dev_test.csv')
    print(f"ğŸ“‹ Total samples: {len(test)}")
    
    # ë°ì´í„° í˜•ì‹ ê²€ì¦
    print("ğŸ” Validating data format...")
    valid_samples = 0
    for idx, row in test.iterrows():
        if validate_data_format(row):
            valid_samples += 1
        else:
            print(f"âŒ Invalid data format at row {idx}")
    
    print(f"âœ… Valid samples: {valid_samples}/{len(test)}")
    
    # ì´ë¯¸ì§€ ë””ë ‰í† ë¦¬ í™•ì¸
    image_dir = './data/input_images/'
    if not os.path.exists(image_dir):
        print(f"âŒ Image directory not found: {image_dir}")
        return
    
    print(f"âœ… Image directory found: {image_dir}")
    available_images = len([f for f in os.listdir(image_dir) if f.endswith('.jpg')])
    print(f"âœ… Available images: {available_images}")
    
    # ì •ë‹µì§€ ë¡œë”© (ì„ íƒì )
    ground_truth = None
    try:
        ground_truth_df = pd.read_csv('./data/dev_ans.csv')
        ground_truth = []
        for _, row in ground_truth_df.iterrows():
            answer_col = str(row.iloc[-1]).strip().upper()
            if answer_col in ['A', 'B', 'C', 'D']:
                ground_truth.append(answer_col)
            elif answer_col == 'ABC':
                ground_truth.append('A')
            elif answer_col == '?':
                ground_truth.append(None)
            else:
                ground_truth.append(None)
        print(f"âœ… Ground truth loaded: {len([x for x in ground_truth if x is not None])}/{len(ground_truth)} valid answers")
    except FileNotFoundError:
        print("âš ï¸ Ground truth file (dev_ans.csv) not found - accuracy will not be calculated")
    except Exception as e:
        print(f"âš ï¸ Error loading ground truth: {e}")
        ground_truth = None
    
    # ì¶”ë¡ 
    print("\nğŸ” Starting inference...")
    results = []
    answer_counts = {'A': 0, 'B': 0, 'C': 0, 'D': 0}
    
    for idx, (_, row) in enumerate(test.iterrows()):
        # ì´ë¯¸ì§€ ê²½ë¡œ ìƒì„±
        img_filename = os.path.basename(row['img_path'])
        image_path = os.path.join(image_dir, img_filename)
        
        # ë°ì´í„° ì¶”ì¶œ
        question = str(row['Question']).strip()
        choices = [str(row[c]).strip() for c in ['A', 'B', 'C', 'D']]
        
        # VQA ì²˜ë¦¬
        answer = process_vqa_sample(models, image_path, question, choices, sample_idx=idx)
        results.append(answer)
        
        # ë‹µë³€ ì„ íƒ ì¹´ìš´íŠ¸
        if answer in answer_counts:
            answer_counts[answer] += 1
    
    print('âœ… Inference completed!')
    
    # ê²°ê³¼ ì €ì¥
    print("\nğŸ“ Saving results...")
    
    current_time = datetime.now().strftime("%Y%m%d_%H%M%S")
    output_filename = f'vqa_blip2_submit_{current_time}.csv'
    
    submission = pd.DataFrame({
        'ID': [f'TEST_{i:03d}' for i in range(len(results))],
        'answer': results
    })
    submission.to_csv(f'./data/{output_filename}', index=False)
    print(f"âœ… Results saved to 'data/{output_filename}'")
    
    # ìµœì¢… ê²°ê³¼ ìš”ì•½
    print("\n" + "="*60)
    print("ğŸ¯ FINAL RESULTS SUMMARY")
    print("="*60)
    
    total_questions = len(results)
    print(f"ğŸ“‹ Total Questions: {total_questions}")
    
    # ë‹µë³€ ë¶„í¬
    print(f"\nğŸ“Š Answer Distribution:")
    for option in ['A', 'B', 'C', 'D']:
        count = answer_counts[option]
        percentage = (count / total_questions * 100) if total_questions > 0 else 0
        print(f"   {option}: {count:2d}íšŒ ({percentage:5.1f}%)")
    
    # ë‹µë³€ íŒ¨í„´ ë¶„ì„
    most_selected = max(answer_counts, key=answer_counts.get)
    least_selected = min(answer_counts, key=answer_counts.get)
    
    print(f"\nğŸ” Answer Pattern Analysis:")
    print(f"   Most Selected: {most_selected} ({answer_counts[most_selected]}íšŒ)")
    print(f"   Least Selected: {least_selected} ({answer_counts[least_selected]}íšŒ)")
    
    # ì „ì²´ ë‹µë³€ ë¦¬ìŠ¤íŠ¸
    print(f"\nğŸ“ All Answers: {' '.join(results)}")
    
    # ì •í™•ë„ ê³„ì‚° (ì •ë‹µì§€ê°€ ìˆëŠ” ê²½ìš°)
    if ground_truth is not None:
        print(f"\nğŸ¯ ACCURACY EVALUATION")
        print("-" * 40)
        
        correct_count = 0
        valid_count = 0
        
        for i, (pred, true) in enumerate(zip(results, ground_truth)):
            if true is not None:
                valid_count += 1
                if pred == true:
                    correct_count += 1
        
        accuracy = (correct_count / valid_count * 100) if valid_count > 0 else 0
        
        print(f"âœ… Correct: {correct_count}/{valid_count} questions")
        print(f"ğŸ¯ Accuracy: {accuracy:.1f}%")
        
        if accuracy >= 80:
            grade = "ğŸ† Excellent"
        elif accuracy >= 70:
            grade = "ğŸ¥‡ Good"
        elif accuracy >= 60:
            grade = "ğŸ¥ˆ Fair"
        elif accuracy >= 50:
            grade = "ğŸ¥‰ Needs Improvement"
        else:
            grade = "âŒ Poor"
        
        print(f"ğŸ“Š Performance Grade: {grade}")
    
    print("="*60)
    print("\nğŸ‰ BLIP2-style VQA inference completed successfully!")

if __name__ == "__main__":
    main() 