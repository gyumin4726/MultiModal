import torch
import pandas as pd
from PIL import Image
import torchvision.transforms as transforms
from tqdm import tqdm
import re
from config import seed_everything
import os
from datetime import datetime

# ëª¨ë“ˆ ì„í¬íŠ¸
from model.vision_encoder import load_vision_encoder
from model.text_encoder import load_vqa_text_encoder
from model.language_model import load_language_model
from model.multimodal_fusion import HierarchicalVQAFusion

def extract_answer_letter(text):
    """LLM ì‘ë‹µì—ì„œ A, B, C, D ë‹µë³€ ì¶”ì¶œ"""
    if not text:
        print(f"âš ï¸ Empty response, defaulting to A")
        return 'A'
    
    text = text.strip().upper()
    
    # ë‹¤ì–‘í•œ íŒ¨í„´ ê²€ìƒ‰ (ìš°ì„ ìˆœìœ„ ìˆœ)
    patterns = [
        r'\b([ABCD])\b(?:\s*[.:]|\s*$)',  # ë‹¨ë… ë¬¸ì + ë§ˆì¹¨í‘œ/ì½œë¡ /ë
        r'ANSWER[:\s]*([ABCD])',           # "ANSWER: A" í˜•íƒœ
        r'SOLUTION[:\s]*([ABCD])',         # "SOLUTION: A" í˜•íƒœ  
        r'([ABCD])[.:]',                   # "A." ë˜ëŠ” "A:" í˜•íƒœ
        r'\b([ABCD])\b',                   # ë‹¨ìˆœ ë¬¸ì ë§¤ì¹­ (ë§ˆì§€ë§‰ ìˆ˜ë‹¨)
    ]
    
    for pattern in patterns:
        matches = re.findall(pattern, text)
        if matches:
            answer = matches[-1]  # ë§ˆì§€ë§‰ ë§¤ì¹˜ ì‚¬ìš©
            print(f"âœ… Extracted answer '{answer}' using pattern: {pattern}")
            return answer
    
    # ëª¨ë“  íŒ¨í„´ ì‹¤íŒ¨ì‹œ ë””ë²„ê¹… ì •ë³´ ì¶œë ¥
    print(f"âŒ No answer pattern found in: '{text[:100]}...'")
    print(f"âš ï¸ Defaulting to A")
    return 'A'

def count_parameters(model):
    """ëª¨ë¸ì˜ ì´ íŒŒë¼ë¯¸í„° ê°œìˆ˜ ê³„ì‚°"""
    if model is None:
        return 0
    return sum(p.numel() for p in model.parameters())

def format_parameter_count(count):
    """íŒŒë¼ë¯¸í„° ê°œìˆ˜ë¥¼ ì½ê¸° ì‰¬ìš´ í˜•íƒœë¡œ ë³€í™˜"""
    if count >= 1e9:
        return f"{count/1e9:.2f}B"
    elif count >= 1e6:
        return f"{count/1e6:.2f}M"
    elif count >= 1e3:
        return f"{count/1e3:.2f}K"
    else:
        return str(count)

def load_models():
    """ëª¨ë“  ëª¨ë¸ ë¡œë”©"""
    print("ğŸš€ Loading MultiModal VQA System...")
    
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    print(f"âœ… Using device: {device}")
    
    models = {
        'vision_encoder': None,
        'text_encoder': None,
        'language_model': None,
        'multimodal_fusion': None,
        'device': device
    }
    
    total_params = 0
    
    # Vision Encoder
    print("ğŸ–¼ï¸ Loading Vision Encoder...")
    vision_encoder = load_vision_encoder(
        model_name='vit_large_patch16_224',  # ìµœê³  ì„±ëŠ¥ ViT ëª¨ë¸
        pretrained=True,  # ImageNet ì‚¬ì „ í›ˆë ¨ ê°€ì¤‘ì¹˜ ì‚¬ìš©
        output_dim=1024,
        frozen_stages=1,
        use_skip_connection=True  # MASC-V í™œì„±í™”
    ).to(device)
    models['vision_encoder'] = vision_encoder
    vision_params = count_parameters(vision_encoder)
    total_params += vision_params
    print(f"âœ… Vision Encoder loaded! Parameters: {format_parameter_count(vision_params)}")
    
    # VQA Text Encoder
    print("ğŸ“ Loading VQA-Optimized Text Encoder...")
    text_encoder = load_vqa_text_encoder(
        model_type='vqa_optimized',
        output_dim=1024,
        device=device
    )
    models['text_encoder'] = text_encoder
    text_params = count_parameters(text_encoder)
    total_params += text_params
    print(f"âœ… VQA Text Encoder loaded! Parameters: {format_parameter_count(text_params)}")
    
    # Language Model
    print("ğŸ¤– Loading Language Model...")
    language_model = load_language_model(
        model_name='microsoft/phi-2',
        device=device
    )
    models['language_model'] = language_model
    llm_params = count_parameters(language_model)
    total_params += llm_params
    print(f"âœ… Language Model loaded! Parameters: {format_parameter_count(llm_params)}")
    
    # VQA MultiModal Fusion
    print("ğŸ”— Loading VQA Hierarchical Fusion...")
    multimodal_fusion = HierarchicalVQAFusion(
        vision_dim=1024,
        text_dim=1024,
        output_dim=1024
    ).to(device)
    models['multimodal_fusion'] = multimodal_fusion
    fusion_params = count_parameters(multimodal_fusion)
    total_params += fusion_params
    print(f"âœ… VQA Hierarchical Fusion loaded! Parameters: {format_parameter_count(fusion_params)}")
    
    # Image Transform
    models['transform'] = transforms.Compose([
        transforms.Resize((224, 224)),
        transforms.ToTensor(),
        transforms.Normalize(mean=[0.485, 0.456, 0.406],
                           std=[0.229, 0.224, 0.225])
    ])
    
    # ì´ íŒŒë¼ë¯¸í„° ê°œìˆ˜ ì¶œë ¥
    print("\nğŸ”¢ Total Parameter Count:")
    print(f"   Combined Models: {format_parameter_count(total_params)} parameters")
    print(f"   Exact Count: {total_params:,} parameters")
    
    return models

def process_vqa_sample(models, image_path, question, choices, sample_idx):
    """ë‹¨ì¼ VQA ìƒ˜í”Œ ì²˜ë¦¬"""
    
    # ë¬¸ì œ ë²ˆí˜¸ ë° êµ¬ë¶„ì„  ì¶œë ¥
    print(f"\n{'='*60}")
    print(f"ğŸ“ Question {sample_idx + 1:02d}/60 - Processing: {image_path}")
    print(f"{'='*60}")
    
    # Vision Features ì¶”ì¶œ
    with torch.no_grad():
        vision_features = models['vision_encoder'](image_path)
    
    # VQA Text Features ì¶”ì¶œ (Question + Choices êµ¬ì¡°í™”)
    with torch.no_grad():
        text_features, qc_attention = models['text_encoder'](question, choices)
    
    # MultiModal Fusion
    with torch.no_grad():
        fused_features = models['multimodal_fusion'](vision_features, text_features)
    
    # í”¼ì²˜ ì •ë³´ ì¶œë ¥ (ë””ë²„ê¹…ìš©)
    features_info = [
        f"Vision({vision_features.shape})",
        f"Text({text_features.shape})",
        f"Fused({fused_features.shape})"
    ]
    
    # LLM ì¶”ë¡  (í”¼ì²˜ í™œìš©)
    print(f"ğŸš€ Using features: {', '.join(features_info)}")
    response = models['language_model'].answer_question_with_features(
        question=question,
        choices=choices,
        vision_features=vision_features,
        text_features=text_features,
        fused_features=fused_features
    )
    answer = extract_answer_letter(response)
    
    # ë””ë²„ê¹…: ì²˜ìŒ 3ê°œ ìƒ˜í”Œì˜ ì‘ë‹µ ì¶œë ¥
    if sample_idx < 3:
        print(f"ğŸ” Sample {sample_idx+1} LLM Response: '{response}'")
        print(f"ğŸ” Response length: {len(response)}")
        print(f"ğŸ¯ Extracted Answer: {answer}")
    
    print(f"âœ… Question {sample_idx + 1:02d} completed â†’ Answer: {answer}")
    print(f"{'='*60}\n")
            
    return answer

def main():
    """ë©”ì¸ ì¶”ë¡  í•¨ìˆ˜"""
    # ì‹œë“œ ê³ ì •
    seed_everything()
    
    print("ğŸš€ Starting MultiModal VQA Inference...")
    print("="*60)
    
    # ëª¨ë¸ ë¡œë”©
    models = load_models()
    
    # ë°ì´í„° ë¡œë”©
    print("\nğŸ“Š Loading test data...")
    test = pd.read_csv('./data/dev_test.csv')
    print(f"ğŸ“‹ Total samples: {len(test)}")
    
    # ì¶”ë¡ 
    print("\nğŸ” Starting inference...")
    results = []
    
    # tqdmì„ disableí•˜ì—¬ progress bar ëŒ€ì‹  ìš°ë¦¬ë§Œì˜ ì¶œë ¥ ì‚¬ìš©
    for idx, (_, row) in enumerate(test.iterrows()):
        # ì´ë¯¸ì§€ ê²½ë¡œ ìˆ˜ì •: ë³´ë‹¤ ì•ˆì „í•œ ê²½ë¡œ ì²˜ë¦¬
        img_filename = os.path.basename(row['img_path'])  # TEST_000.jpg ì¶”ì¶œ 
        image_path = os.path.join('./data/input_images/', img_filename)
        question = row['Question']
        choices = [row[c] for c in ['A', 'B', 'C', 'D']]
        
        # VQA ì²˜ë¦¬
        answer = process_vqa_sample(models, image_path, question, choices, sample_idx=idx)
        results.append(answer)
    
    print('âœ… Inference completed!')
    
    # ê²°ê³¼ ì €ì¥
    print("\nğŸ“ Saving results...")
    
    # í˜„ì¬ ë‚ ì§œì™€ ì‹œê°„ìœ¼ë¡œ íŒŒì¼ëª… ìƒì„±
    current_time = datetime.now().strftime("%Y%m%d_%H%M%S")
    output_filename = f'vqa_enhanced_submit_{current_time}.csv'
    
    submission = pd.DataFrame({
        'ID': [f'TEST_{i:03d}' for i in range(len(results))],
        'answer': results
    })
    submission.to_csv(f'./data/{output_filename}', index=False)
    print(f"âœ… Results saved to 'data/{output_filename}'")
    
    # ê²°ê³¼ ë¶„ì„
    print(f"\nğŸ“Š Answer distribution:")
    for answer in ['A', 'B', 'C', 'D']:
        count = results.count(answer)
        print(f"   {answer}: {count} ({count/len(results)*100:.1f}%)")
    
    print("\nğŸ‰ Enhanced VQA inference completed successfully!")

if __name__ == "__main__":
    main() 