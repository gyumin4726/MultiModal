import torch
import torch.nn as nn
from transformers import AutoTokenizer, AutoModelForCausalLM
import os

# ìºì‹œ ë””ë ‰í† ë¦¬ ìƒì„±
os.makedirs("./model_cache", exist_ok=True)


class MultimodalLanguageModel(nn.Module):
    """BLIP2 ìŠ¤íƒ€ì¼ ë©€í‹°ëª¨ë‹¬ ì–¸ì–´ ëª¨ë¸ - OPT ê¸°ë°˜"""
    
    def __init__(self, model_name="facebook/opt-2.7b", device=None):
        super().__init__()
        
        self.model_name = model_name
        self.device = device or ('cuda' if torch.cuda.is_available() else 'cpu')
        
        # í† í¬ë‚˜ì´ì € ë¡œë”©
        print(f"Loading multimodal tokenizer: {model_name}")
        self.tokenizer = AutoTokenizer.from_pretrained(
            model_name,
            cache_dir="./model_cache",
            local_files_only=False
        )
        if self.tokenizer.pad_token is None:
            self.tokenizer.pad_token = self.tokenizer.eos_token
            
        # OPT ì–¸ì–´ ëª¨ë¸ ë¡œë”©
        print(f"Loading multimodal language model: {model_name}")
        self.model = AutoModelForCausalLM.from_pretrained(
            model_name,
            torch_dtype=torch.float16 if self.device == 'cuda' else torch.float32,
            cache_dir="./model_cache",
            local_files_only=False,
            trust_remote_code=True
        )
        self.model.to(self.device)
        
        # ğŸ”¥ í•µì‹¬: Vision Token Projection Layer
        # Q-Former ì¶œë ¥(768D)ì„ LLM ì„ë² ë”© ì°¨ì›ìœ¼ë¡œ ë³€í™˜
        self.vision_token_proj = nn.Linear(768, self.model.config.hidden_size).to(self.device)
        
        print(f"âœ… Multimodal LLM loaded successfully on {self.device}")
        print(f"âœ… Vision token projection: 768 â†’ {self.model.config.hidden_size}")
    
    def forward_with_vision_tokens(self, vision_tokens, text_input_ids, attention_mask=None):
        """ë¹„ì „ í† í°ê³¼ í…ìŠ¤íŠ¸ë¥¼ í•¨ê»˜ ì²˜ë¦¬ - BLIP2 ë°©ì‹"""
        
        batch_size = text_input_ids.shape[0]
        
        # 1. ë¹„ì „ í† í°ì„ LLM ì„ë² ë”© ì°¨ì›ìœ¼ë¡œ ë³€í™˜
        vision_embeddings = self.vision_token_proj(vision_tokens)  # [1, 32, hidden_size]
        
        # 2. í…ìŠ¤íŠ¸ í† í°ì„ ì„ë² ë”©ìœ¼ë¡œ ë³€í™˜
        text_embeddings = self.model.get_input_embeddings()(text_input_ids)  # [1, seq_len, hidden_size]
        
        # 3. ë¹„ì „ í† í°ê³¼ í…ìŠ¤íŠ¸ í† í°ì„ ê²°í•©
        # [Vision Tokens] + [Text Tokens] ìˆœì„œë¡œ ê²°í•©
        combined_embeddings = torch.cat([vision_embeddings, text_embeddings], dim=1)
        
        # 4. Attention maskë„ í™•ì¥
        if attention_mask is not None:
            vision_attention_mask = torch.ones(batch_size, vision_tokens.shape[1], device=self.device)
            combined_attention_mask = torch.cat([vision_attention_mask, attention_mask], dim=1)
        else:
            combined_attention_mask = None
        
        # 5. LLM forward pass (ì„ë² ë”©ì„ ì§ì ‘ ì…ë ¥)
        outputs = self.model(
            inputs_embeds=combined_embeddings,
            attention_mask=combined_attention_mask,
            use_cache=False
        )
        
        return outputs
    
    def generate_with_vision(self, vision_tokens, text_prompt, max_new_tokens=50, temperature=0.0):
        """ë¹„ì „ í† í°ê³¼ í•¨ê»˜ í…ìŠ¤íŠ¸ ìƒì„±"""
        
        # í…ìŠ¤íŠ¸ í† í¬ë‚˜ì´ì§•
        text_inputs = self.tokenizer(text_prompt, return_tensors="pt").to(self.device)
        
        batch_size = text_inputs['input_ids'].shape[0]
        
        # ë¹„ì „ í† í°ì„ ì„ë² ë”©ìœ¼ë¡œ ë³€í™˜
        vision_embeddings = self.vision_token_proj(vision_tokens)  # [1, 32, hidden_size]
        
        # í…ìŠ¤íŠ¸ ì„ë² ë”©
        text_embeddings = self.model.get_input_embeddings()(text_inputs['input_ids'])
        
        # ê²°í•©ëœ ì„ë² ë”©
        combined_embeddings = torch.cat([vision_embeddings, text_embeddings], dim=1)
        
        # Attention mask ìƒì„±
        vision_attention_mask = torch.ones(batch_size, vision_tokens.shape[1], device=self.device)
        combined_attention_mask = torch.cat([vision_attention_mask, text_inputs['attention_mask']], dim=1)
        
        # ìƒì„± ì„¤ì •
        generation_config = {
            "max_new_tokens": max_new_tokens,
            "pad_token_id": self.tokenizer.pad_token_id,
            "eos_token_id": self.tokenizer.eos_token_id,
            "do_sample": temperature > 0,
        }
        
        if temperature > 0:
            generation_config.update({
                "temperature": temperature,
                "top_p": 0.9
            })
        
        # ğŸ”¥ í•µì‹¬: ì„ë² ë”©ì„ ì‚¬ìš©í•œ ìƒì„±
        with torch.no_grad():
            outputs = self.model.generate(
                inputs_embeds=combined_embeddings,
                attention_mask=combined_attention_mask,
                **generation_config
            )
        
        # ìƒì„±ëœ ë¶€ë¶„ë§Œ ë””ì½”ë”© (ì›ë³¸ ì…ë ¥ ì œì™¸)
        generated_tokens = outputs[0][combined_embeddings.shape[1]:]
        generated_text = self.tokenizer.decode(generated_tokens, skip_special_tokens=True)
        
        return generated_text.strip()
    
    def answer_vqa_multimodal(self, vision_tokens, question, choices):
        """ì§„ì§œ ë©€í‹°ëª¨ë‹¬ VQA - ë¹„ì „ í† í°ì„ ì§ì ‘ ì²˜ë¦¬"""
        
        # ì„ íƒì§€ í…ìŠ¤íŠ¸ ìƒì„±
        choices_text = "\n".join([f"{chr(65+i)}. {choice}" for i, choice in enumerate(choices)])
        
        # ëª…í™•í•œ í˜•ì‹ ì§€ì • í”„ë¡¬í”„íŠ¸
        prompt = f"""Based on the image, answer the following question.

Question: {question}

Options:
{choices_text}

Instructions: Choose the best answer from the options above. Respond with ONLY the letter (A, B, C, or D) followed by a period.

Answer:"""
        
        # ë¹„ì „ í† í°ê³¼ í•¨ê»˜ ìƒì„±
        response = self.generate_with_vision(
            vision_tokens=vision_tokens,
            text_prompt=prompt,
            max_new_tokens=5,  # ë” ì§§ê²Œ ì„¤ì • (A. í˜•ì‹ë§Œ í•„ìš”)
            temperature=0.0
        )
        
        return response


def load_language_model(**kwargs):
    """ì–¸ì–´ ëª¨ë¸ ë¡œë”© í•¨ìˆ˜ - BLIP2 ìŠ¤íƒ€ì¼ë§Œ ì§€ì›"""
    return MultimodalLanguageModel(**kwargs) 