import torch
import torch.nn as nn
import torch.nn.functional as F


class VQAMultiModalFusion(nn.Module):
    """VQA íŠ¹í™” ê³ ê¸‰ ë©€í‹°ëª¨ë‹¬ ìœµí•© ëª¨ë“ˆ"""
    
    def __init__(self, 
                 vision_dim=1024, 
                 text_dim=1024, 
                 hidden_dim=512, 
                 output_dim=1024,
                 num_heads=8):
        super().__init__()
        
        self.vision_dim = vision_dim
        self.text_dim = text_dim
        self.hidden_dim = hidden_dim
        self.output_dim = output_dim
        self.num_heads = num_heads
        
        # 1. ëª¨ë‹¬ë¦¬í‹°ë³„ Feature Enhancement
        self.vision_enhancer = nn.Sequential(
            nn.Linear(vision_dim, hidden_dim),
            nn.LayerNorm(hidden_dim),
            nn.ReLU(),
            nn.Dropout(0.1),
            nn.Linear(hidden_dim, hidden_dim),
            nn.LayerNorm(hidden_dim)
        )
        
        self.text_enhancer = nn.Sequential(
            nn.Linear(text_dim, hidden_dim),
            nn.LayerNorm(hidden_dim),
            nn.ReLU(),
            nn.Dropout(0.1),
            nn.Linear(hidden_dim, hidden_dim),
            nn.LayerNorm(hidden_dim)
        )
        
        # 2. Bi-directional Cross-Attention
        self.v2t_attention = nn.MultiheadAttention(
            embed_dim=hidden_dim,
            num_heads=num_heads,
            dropout=0.1,
            batch_first=True
        )
        
        self.t2v_attention = nn.MultiheadAttention(
            embed_dim=hidden_dim,
            num_heads=num_heads,
            dropout=0.1,
            batch_first=True
        )
        
        # 3. Adaptive Fusion Weights (í•™ìŠµ ê°€ëŠ¥í•œ ê°€ì¤‘ì¹˜)
        self.fusion_gate = nn.Sequential(
            nn.Linear(hidden_dim * 2, hidden_dim),
            nn.ReLU(),
            nn.Linear(hidden_dim, 3),  # vision, text, interaction ê°€ì¤‘ì¹˜
            nn.Softmax(dim=-1)
        )
        
        # 4. Multi-scale Feature Fusion
        self.multi_scale_fusion = nn.ModuleList([
            nn.Sequential(
                nn.Linear(hidden_dim, hidden_dim // (2**i)),
                nn.ReLU(),
                nn.Linear(hidden_dim // (2**i), hidden_dim)
            ) for i in range(3)  # 3ê°œì˜ ìŠ¤ì¼€ì¼
        ])
        
        # 5. Final Output Layer
        self.output_layer = nn.Sequential(
            nn.Linear(hidden_dim * 4, output_dim),  # enhanced + attended + gated + multi-scale
            nn.LayerNorm(output_dim),
            nn.ReLU(),
            nn.Dropout(0.1),
            nn.Linear(output_dim, output_dim),
            nn.LayerNorm(output_dim)
        )
        
        print(f"VQA MultiModal Fusion initialized: V{vision_dim}+T{text_dim} -> {output_dim}")
    
    def forward(self, vision_features, text_features):
        """
        Args:
            vision_features: (B, vision_dim) - ë¹„ì „ í”¼ì²˜
            text_features: (B, text_dim) - í…ìŠ¤íŠ¸ í”¼ì²˜ (VQA êµ¬ì¡°í™”ëœ)
            
        Returns:
            fused_features: (B, output_dim) - ê³ ê¸‰ ìœµí•©ëœ í”¼ì²˜
        """
        batch_size = vision_features.size(0)
        
        # 1. Feature Enhancement
        v_enhanced = self.vision_enhancer(vision_features)  # (B, hidden_dim)
        t_enhanced = self.text_enhancer(text_features)      # (B, hidden_dim)
        
        # 2. Sequence ì°¨ì› ì¶”ê°€ (attentionì„ ìœ„í•´)
        v_seq = v_enhanced.unsqueeze(1)  # (B, 1, hidden_dim)
        t_seq = t_enhanced.unsqueeze(1)  # (B, 1, hidden_dim)
        
        # 3. Bi-directional Cross-Attention
        v2t_attended, v2t_weights = self.v2t_attention(v_seq, t_seq, t_seq)  # Vision queries Text
        t2v_attended, t2v_weights = self.t2v_attention(t_seq, v_seq, v_seq)  # Text queries Vision
        
        v2t_attended = v2t_attended.squeeze(1)  # (B, hidden_dim)
        t2v_attended = t2v_attended.squeeze(1)  # (B, hidden_dim)
        
        # 4. Adaptive Fusion Gating
        gate_input = torch.cat([v2t_attended, t2v_attended], dim=-1)  # (B, hidden_dim*2)
        fusion_weights = self.fusion_gate(gate_input)  # (B, 3)
        
        # ê°€ì¤‘ì¹˜ ì ìš©
        w_v, w_t, w_i = fusion_weights[:, 0:1], fusion_weights[:, 1:2], fusion_weights[:, 2:3]
        
        gated_features = (
            w_v * v_enhanced +           # ì›ë³¸ vision
            w_t * t_enhanced +           # ì›ë³¸ text  
            w_i * (v2t_attended + t2v_attended) / 2  # ìƒí˜¸ì‘ìš©
        )  # (B, hidden_dim)
        
        # 5. Multi-scale Feature Processing
        multi_scale_features = []
        for scale_layer in self.multi_scale_fusion:
            scale_feat = scale_layer(gated_features)
            multi_scale_features.append(scale_feat)
        
        multi_scale_combined = torch.stack(multi_scale_features, dim=1).mean(dim=1)  # (B, hidden_dim)
        
        # 6. Final Fusion
        final_input = torch.cat([
            v_enhanced,           # ê°•í™”ëœ vision
            t_enhanced,           # ê°•í™”ëœ text
            gated_features,       # ê²Œì´íŠ¸ëœ ìœµí•©
            multi_scale_combined  # ë©€í‹°ìŠ¤ì¼€ì¼ ìœµí•©
        ], dim=-1)  # (B, hidden_dim * 4)
        
        fused_features = self.output_layer(final_input)  # (B, output_dim)
        
        return fused_features


class HierarchicalVQAFusion(nn.Module):
    """ê³„ì¸µì  VQA ìœµí•© - ë‹¨ê³„ë³„ ì •ë³´ ìœµí•©"""
    
    def __init__(self, vision_dim=1024, text_dim=1024, output_dim=1024):
        super().__init__()
        
        # Level 1: Low-level feature alignment
        self.level1_fusion = VQAMultiModalFusion(
            vision_dim=vision_dim,
            text_dim=text_dim,
            hidden_dim=256,
            output_dim=512,
            num_heads=4
        )
        
        # Level 2: High-level semantic fusion
        self.level2_fusion = VQAMultiModalFusion(
            vision_dim=512,  # from level 1
            text_dim=text_dim,
            hidden_dim=512,
            output_dim=output_dim,
            num_heads=8
        )
        
        # Residual connection
        self.residual_proj = nn.Linear(vision_dim + text_dim, output_dim)
        
    def forward(self, vision_features, text_features):
        """ê³„ì¸µì  ìœµí•©"""
        
        # Level 1: ê¸°ë³¸ ìœµí•©
        level1_fused = self.level1_fusion(vision_features, text_features)  # (B, 512)
        
        # Level 2: ê³ ê¸‰ ìœµí•© (Level 1 ê²°ê³¼ + ì›ë³¸ í…ìŠ¤íŠ¸)
        level2_fused = self.level2_fusion(level1_fused, text_features)  # (B, output_dim)
        
        # Residual connection
        residual = self.residual_proj(torch.cat([vision_features, text_features], dim=-1))
        
        final_output = level2_fused + residual  # Skip connection
        
        return final_output


class EnhancedMultiModalProcessor:
    """í–¥ìƒëœ ë©€í‹°ëª¨ë‹¬ ì²˜ë¦¬ê¸° - VQA íŠ¹í™”"""
    
    def __init__(self, vision_encoder, text_encoder, language_model, fusion_type='hierarchical'):
        self.vision_encoder = vision_encoder
        self.text_encoder = text_encoder  
        self.language_model = language_model
        self.fusion_type = fusion_type
        
        # ìœµí•© ëª¨ë“ˆ ì„ íƒ
        if fusion_type == 'hierarchical':
            self.fusion_model = HierarchicalVQAFusion(
                vision_dim=1024,
                text_dim=1024,
                output_dim=1024
            )
        else:  # 'advanced'
            self.fusion_model = VQAMultiModalFusion(
                vision_dim=1024,
                text_dim=1024,
                hidden_dim=512,
                output_dim=1024
            )
        
        if torch.cuda.is_available():
            self.fusion_model = self.fusion_model.cuda()
            
        print(f"âœ… Enhanced MultiModal Processor initialized with {fusion_type} fusion")
            
    def process_multimodal_input(self, image_tensor, question_text, choices):
        """VQA íŠ¹í™” ë©€í‹°ëª¨ë‹¬ ì…ë ¥ ì²˜ë¦¬"""
        
        # 1. Vision í”¼ì²˜ ì¶”ì¶œ
        with torch.no_grad():
            vision_features = self.vision_encoder(image_tensor)  # (1, 1024)
        
        # 2. VQA êµ¬ì¡°í™” í…ìŠ¤íŠ¸ ì¸ì½”ë”©
        text_features, qc_attention = self.text_encoder(question_text, choices)  # (1, 1024)
        
        # 3. ê³ ê¸‰ ë©€í‹°ëª¨ë‹¬ ìœµí•©
        fused_features = self.fusion_model(vision_features, text_features)  # (1, 1024)
        
        # 4. ì–¸ì–´ ëª¨ë¸ë¡œ VQA ì¶”ë¡ 
        answer = self.language_model.answer_question_with_features(
            question_text, 
            choices, 
            vision_features=vision_features,
            text_features=text_features,
            fused_features=fused_features
        )
        
        return answer, {
            'vision_features': vision_features,
            'text_features': text_features, 
            'fused_features': fused_features,
            'qc_attention': qc_attention
        }


if __name__ == "__main__":
    print("ğŸš€ Enhanced VQA MultiModal System:")
    print("="*60)
    
    print("\n1ï¸âƒ£ Vision Processing:")
    print("   ğŸ“¸ Image â†’ ViT-Large â†’ MASC-V â†’ 1024D vision features")
    
    print("\n2ï¸âƒ£ VQA Text Processing:")
    print("   ğŸ“ Question + Choices â†’ MPNet â†’ Question-Choice Attention â†’ 1024D text features")
    
    print("\n3ï¸âƒ£ Hierarchical Fusion:")
    print("   ğŸ”— Level 1: Low-level alignment (Vision â†” Text)")
    print("   ğŸ”— Level 2: High-level semantic fusion + Residual")
    print("   ğŸ”— Result: 1024D fused features")
    
    print("\n4ï¸âƒ£ Advanced Reasoning:")
    print("   ğŸ§  Fused features â†’ Question-type specific prompts â†’ Self-verification â†’ Answer")
    
    print("\nâœ¨ ì£¼ìš” ê°œì„ ì‚¬í•­:")
    print("   ğŸ”¥ Question-Choice êµ¬ì¡°í™” ì¸ì½”ë”©")
    print("   ğŸ”¥ Bi-directional Cross-Attention")
    print("   ğŸ”¥ Adaptive Fusion Gating")
    print("   ğŸ”¥ Multi-scale Feature Processing")
    print("   ğŸ”¥ Hierarchical Fusion Architecture")
    print("   ğŸ”¥ VQA-specific Prompt Engineering")
    
    # ê³ ê¸‰ ìœµí•© ëª¨ë“ˆ í…ŒìŠ¤íŠ¸
    fusion = VQAMultiModalFusion(vision_dim=1024, text_dim=1024, output_dim=1024)
    
    # ë”ë¯¸ ë°ì´í„°ë¡œ í…ŒìŠ¤íŠ¸
    dummy_vision = torch.randn(2, 1024)
    dummy_text = torch.randn(2, 1024)
    
    fused = fusion(dummy_vision, dummy_text)
    print(f"\nğŸ§ª Test: Vision{dummy_vision.shape} + Text{dummy_text.shape} â†’ Fused{fused.shape}")
    
    # ê³„ì¸µì  ìœµí•© í…ŒìŠ¤íŠ¸
    hierarchical_fusion = HierarchicalVQAFusion(vision_dim=1024, text_dim=1024, output_dim=1024)
    hierarchical_fused = hierarchical_fusion(dummy_vision, dummy_text)
    print(f"ğŸ§ª Hierarchical: Vision{dummy_vision.shape} + Text{dummy_text.shape} â†’ Fused{hierarchical_fused.shape}") 