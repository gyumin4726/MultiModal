import torch
import torch.nn as nn
import torch.nn.functional as F
from transformers import AutoTokenizer, AutoModel


class VQATextEncoder(nn.Module):
    """VQA íƒœìŠ¤í¬ íŠ¹í™” í…ìŠ¤íŠ¸ ì¸ì½”ë” - Questionê³¼ Choicesë¥¼ êµ¬ì¡°ì ìœ¼ë¡œ ì²˜ë¦¬"""
    
    def __init__(self, 
                 model_name='sentence-transformers/all-mpnet-base-v2',
                 output_dim=1024,
                 device=None,
                 max_length=256):
        super().__init__()
        
        self.model_name = model_name
        self.output_dim = output_dim
        self.max_length = max_length
        self.device = device or ('cuda' if torch.cuda.is_available() else 'cpu')
        
        # ê³ ì„±ëŠ¥ transformer ëª¨ë¸ ë¡œë”©
        print(f"Loading VQA-optimized text encoder: {model_name}")
        self.tokenizer = AutoTokenizer.from_pretrained(model_name)
        self.model = AutoModel.from_pretrained(model_name)
        self.model.to(self.device)
        self.model.eval()
        
        self.native_dim = self.model.config.hidden_size
        print(f"Native embedding dimension: {self.native_dim}")
        
        # VQA íŠ¹í™” êµ¬ì¡°
        # 1. Question ì¸ì½”ë”
        self.question_proj = nn.Linear(self.native_dim, output_dim // 2).to(self.device)
        
        # 2. Choices ì¸ì½”ë” (4ê°œ ì„ íƒì§€)
        self.choice_proj = nn.Linear(self.native_dim, output_dim // 8).to(self.device)  # ê° ì„ íƒì§€ë‹¹ 128ì°¨ì›
        
        # 3. Question-Choice ìƒí˜¸ì‘ìš© ëª¨ë“ˆ
        self.qc_attention = nn.MultiheadAttention(
            embed_dim=output_dim // 8,
            num_heads=4,
            dropout=0.1,
            batch_first=True
        ).to(self.device)
        
        # 4. ìµœì¢… ìœµí•© ë ˆì´ì–´
        self.fusion_layer = nn.Sequential(
            nn.Linear(output_dim // 2 + output_dim // 2, output_dim),  # question + attended_choices
            nn.LayerNorm(output_dim),
            nn.ReLU(),
            nn.Dropout(0.1),
            nn.Linear(output_dim, output_dim),
            nn.LayerNorm(output_dim)
        ).to(self.device)
        
        print(f"VQA Text Encoder initialized: {self.native_dim} -> {output_dim}")
    
    def encode_single_text(self, text):
        """ë‹¨ì¼ í…ìŠ¤íŠ¸ ì¸ì½”ë”©"""
        if isinstance(text, str):
            text = [text]
        
        encoded_input = self.tokenizer(
            text, 
            padding=True, 
            truncation=True, 
            max_length=self.max_length,
            return_tensors='pt'
        )
        
        encoded_input = {k: v.to(self.device) for k, v in encoded_input.items()}
        
        with torch.no_grad():
            outputs = self.model(**encoded_input)
            # CLS token ë˜ëŠ” mean pooling
            if hasattr(outputs, 'pooler_output') and outputs.pooler_output is not None:
                embeddings = outputs.pooler_output
            else:
                embeddings = outputs.last_hidden_state.mean(dim=1)
            
            embeddings = F.normalize(embeddings, p=2, dim=1)
        
        return embeddings
    
    def encode_vqa_structured(self, question, choices):
        """VQA êµ¬ì¡°í™” ì¸ì½”ë”© - Questionê³¼ Choicesë¥¼ ë”°ë¡œ ì²˜ë¦¬ í›„ ìœµí•©"""
        
        # 1. Question ì¸ì½”ë”©
        question_emb = self.encode_single_text(question)  # (1, native_dim)
        question_feat = self.question_proj(question_emb)  # (1, output_dim//2)
        
        # 2. ê° Choice ì¸ì½”ë”©
        choice_embeddings = []
        for choice in choices:
            choice_emb = self.encode_single_text(choice)  # (1, native_dim)
            choice_feat = self.choice_proj(choice_emb)    # (1, output_dim//8)
            choice_embeddings.append(choice_feat)
        
        # 3. Choicesë¥¼ í•˜ë‚˜ì˜ í…ì„œë¡œ ê²°í•©
        choices_tensor = torch.stack(choice_embeddings, dim=1)  # (1, 4, output_dim//8)
        
        # 4. Questionì„ queryë¡œ, Choicesë¥¼ key/valueë¡œ attention
        question_query = question_feat[:, :self.output_dim//8].unsqueeze(1)  # (1, 1, output_dim//8)
        
        attended_choices, attention_weights = self.qc_attention(
            question_query, choices_tensor, choices_tensor
        )  # (1, 1, output_dim//8)
        
        # 5. ëª¨ë“  choice ì •ë³´ë¥¼ ê²°í•©
        all_choices = choices_tensor.flatten(start_dim=1)  # (1, 4 * output_dim//8)
        
        # 6. Question + Attended Choices ìœµí•©
        final_input = torch.cat([
            question_feat,  # (1, output_dim//2)
            all_choices     # (1, output_dim//2)
        ], dim=-1)
        
        final_features = self.fusion_layer(final_input)  # (1, output_dim)
        
        return final_features, attention_weights.squeeze()
    
    def forward(self, question, choices):
        """VQA í…ìŠ¤íŠ¸ ì¸ì½”ë”©"""
        return self.encode_vqa_structured(question, choices)


class ContextualTextEncoder(nn.Module):
    """ì»¨í…ìŠ¤íŠ¸ ê°•í™” í…ìŠ¤íŠ¸ ì¸ì½”ë” - ì§ˆë¬¸ ìœ í˜•ë³„ íŠ¹í™”"""
    
    def __init__(self, base_encoder):
        super().__init__()
        self.base_encoder = base_encoder
        self.output_dim = base_encoder.output_dim
        
        # ì§ˆë¬¸ ìœ í˜•ë³„ ì»¨í…ìŠ¤íŠ¸ ì„ë² ë”©
        self.context_embeddings = nn.Embedding(10, self.output_dim // 4).to(base_encoder.device)  # 10ê°€ì§€ ì§ˆë¬¸ ìœ í˜•
        
        # ì»¨í…ìŠ¤íŠ¸ ìœµí•© ë ˆì´ì–´
        self.context_fusion = nn.Sequential(
            nn.Linear(self.output_dim + self.output_dim // 4, self.output_dim),
            nn.LayerNorm(self.output_dim),
            nn.ReLU(),
            nn.Linear(self.output_dim, self.output_dim)
        ).to(base_encoder.device)
        
        # ì§ˆë¬¸ ìœ í˜• ë¶„ë¥˜ê¸°
        self.question_type_map = {
            'visual_attributes': 0,
            'object_scene_recognition': 1,
            'activity_behavior': 2,
            'inference_reasoning': 3,
            'contextual_understanding': 4,
            'knowledge_based': 5,
            'comparison_selection': 6,
            'quantitative_analysis': 7,
            'general_visual_qa': 8,
            'default': 9
        }
    
    def get_question_type_id(self, question):
        """ì§ˆë¬¸ ìœ í˜• ID ë°˜í™˜"""
        question_lower = question.lower()
        
        if any(word in question_lower for word in ['color', 'size', 'shape']):
            return self.question_type_map['visual_attributes']
        elif any(word in question_lower for word in ['what is', 'what are', 'identify']):
            return self.question_type_map['object_scene_recognition']
        elif any(word in question_lower for word in ['doing', 'activity', 'behavior']):
            return self.question_type_map['activity_behavior']
        elif any(word in question_lower for word in ['might', 'likely', 'purpose']):
            return self.question_type_map['inference_reasoning']
        elif any(word in question_lower for word in ['where', 'when', 'time']):
            return self.question_type_map['contextual_understanding']
        elif any(word in question_lower for word in ['common', 'typical', 'culture']):
            return self.question_type_map['knowledge_based']
        elif any(word in question_lower for word in ['which', 'best', 'most']):
            return self.question_type_map['comparison_selection']
        elif any(word in question_lower for word in ['how many', 'count']):
            return self.question_type_map['quantitative_analysis']
        else:
            return self.question_type_map['general_visual_qa']
    
    def forward(self, question, choices):
        """ì»¨í…ìŠ¤íŠ¸ ê°•í™” ì¸ì½”ë”©"""
        # 1. ê¸°ë³¸ VQA ì¸ì½”ë”©
        base_features, attention_weights = self.base_encoder(question, choices)
        
        # 2. ì§ˆë¬¸ ìœ í˜•ë³„ ì»¨í…ìŠ¤íŠ¸ ì¶”ê°€
        question_type_id = self.get_question_type_id(question)
        context_emb = self.context_embeddings(torch.tensor(question_type_id).to(base_features.device))
        context_emb = context_emb.unsqueeze(0)  # (1, output_dim//4)
        
        # 3. ì»¨í…ìŠ¤íŠ¸ ìœµí•©
        enhanced_input = torch.cat([base_features, context_emb], dim=-1)
        enhanced_features = self.context_fusion(enhanced_input)
        
        return enhanced_features, attention_weights


def load_vqa_text_encoder(model_type='vqa_optimized', **kwargs):
    """VQA íŠ¹í™” í…ìŠ¤íŠ¸ ì¸ì½”ë” ë¡œë”©"""
    
    if model_type == 'vqa_optimized':
        base_encoder = VQATextEncoder(**kwargs)
        return ContextualTextEncoder(base_encoder)
    else:
        return VQATextEncoder(**kwargs)


if __name__ == "__main__":
    print("ğŸš€ VQA-Optimized Text Encoder Test")
    print("="*50)
    
    # VQA íŠ¹í™” ì¸ì½”ë” í…ŒìŠ¤íŠ¸
    encoder = load_vqa_text_encoder('vqa_optimized', output_dim=1024)
    
    test_question = "What color is the car in the image?"
    test_choices = ["Red", "Blue", "Green", "Yellow"]
    
    features, attention = encoder(test_question, test_choices)
    print(f"âœ… VQA Features shape: {features.shape}")
    print(f"âœ… Attention weights: {attention.shape}")
    print(f"âœ… Question-Choice attention: {attention}") 