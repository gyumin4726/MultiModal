import torch
import torch.nn as nn
import torch.nn.functional as F
import timm
from typing import List, Optional





class VisionEncoder(nn.Module):
    """BLIP2 ìŠ¤íƒ€ì¼ Q-Former Vision Encoder"""
    
    def __init__(self, 
                 model_name: str = 'vit_base_patch16_224',
                 pretrained: bool = True,
                 output_dim: int = 768,
                 frozen_stages: int = 1,
                 num_query_tokens: int = 32):
        super().__init__()
        
        self.model_name = model_name
        self.output_dim = output_dim
        self.num_query_tokens = num_query_tokens
        
        # ViT ë°±ë³¸
        print(f"Loading Vision Transformer: {model_name}")
        self.backbone = timm.create_model(
            model_name,
            pretrained=pretrained,
            num_classes=0,
            global_pool='',
        )
        
        native_dim = self.backbone.embed_dim
        
        # ğŸ”¥ BLIP2 ìŠ¤íƒ€ì¼ Q-Former êµ¬í˜„
        # 1. Learnable Query Tokens (í•™ìŠµ ê°€ëŠ¥í•œ ì¿¼ë¦¬ë“¤)
        self.query_tokens = nn.Parameter(torch.zeros(1, num_query_tokens, native_dim))
        nn.init.normal_(self.query_tokens, mean=0.0, std=0.02)
        
        # 2. Cross-Attention Layer (ì´ë¯¸ì§€ íŒ¨ì¹˜ë“¤ê³¼ ì¿¼ë¦¬ë“¤ ê°„ ìƒí˜¸ì‘ìš©)
        self.cross_attention = nn.MultiheadAttention(
            embed_dim=native_dim,
            num_heads=12,  # ViTì™€ ë™ì¼
            dropout=0.1,
            batch_first=True
        )
        
        # 3. Query Transformer (ì¿¼ë¦¬ë“¤ì„ ë” ì •êµí•˜ê²Œ ì²˜ë¦¬)
        self.query_transformer = nn.TransformerEncoder(
            nn.TransformerEncoderLayer(
                d_model=native_dim,
                nhead=12,
                dim_feedforward=native_dim * 4,
                dropout=0.1,
                batch_first=True
            ),
            num_layers=2  # ê°„ë‹¨í•˜ê²Œ 2ë ˆì´ì–´
        )
        
        # 4. Vision-Language Projection (LLM ì°¨ì›ìœ¼ë¡œ ë³€í™˜)
        self.vision_language_proj = nn.Sequential(
            nn.Linear(native_dim, output_dim),
            nn.LayerNorm(output_dim),
            nn.GELU(),
            nn.Linear(output_dim, output_dim)
        )
        
        # ê¸°ë³¸ ì „ì²˜ë¦¬ ì„¤ì •
        self.preprocess = self._get_preprocessing()
        
        # ì¼ë¶€ ë ˆì´ì–´ freeze
        if frozen_stages > 0:
            self._freeze_stages(frozen_stages)
        
        print(f"âœ… BLIP2-style Q-Former initialized:")
        print(f"   - {num_query_tokens} learnable query tokens")
        print(f"   - Cross-attention with image patches")
        print(f"   - Query transformer processing")
        print(f"   - Vision-Language projection: {native_dim} -> {output_dim}")
    
    def _freeze_stages(self, frozen_stages: int):
        """ì§€ì •ëœ ìˆ˜ë§Œí¼ ì´ˆê¸° ë ˆì´ì–´ë“¤ì„ freeze"""
        # Patch embedding freeze
        if frozen_stages >= 1:
            for param in self.backbone.patch_embed.parameters():
                param.requires_grad = False
            print(f"â„ï¸ Frozen patch embedding")
        
        # Transformer blocks freeze
        if hasattr(self.backbone, 'blocks'):
            num_blocks_to_freeze = min(frozen_stages - 1, len(self.backbone.blocks))
            for i in range(num_blocks_to_freeze):
                for param in self.backbone.blocks[i].parameters():
                    param.requires_grad = False
            
            if num_blocks_to_freeze > 0:
                print(f"â„ï¸ Frozen first {num_blocks_to_freeze} transformer blocks")
    
    def _get_preprocessing(self):
        """ê¸°ë³¸ ì „ì²˜ë¦¬ ì„¤ì •"""
        from torchvision import transforms
        
        # ë‹¨ì¼ í•´ìƒë„ ì‚¬ìš© (ë³µì¡í•œ ë©€í‹°ìŠ¤ì¼€ì¼ ì œê±°)
        size = 224 if 'base' in self.model_name else 384
        
        return transforms.Compose([
            transforms.Resize(size, interpolation=transforms.InterpolationMode.BICUBIC),
            transforms.CenterCrop(size),
            transforms.ToTensor(),
            transforms.Normalize(
                mean=[0.485, 0.456, 0.406],
                std=[0.229, 0.224, 0.225]
            )
        ])

    def forward_qformer(self, x):
        """BLIP2 ìŠ¤íƒ€ì¼ Q-Former ì²˜ë¦¬"""
        # ì´ë¯¸ì§€ ë¡œë”© ë° ì „ì²˜ë¦¬
        if isinstance(x, str):
            from PIL import Image
            image = Image.open(x).convert('RGB')
            processed = self.preprocess(image).unsqueeze(0)
        else:
            processed = x
            
        if torch.cuda.is_available():
            processed = processed.cuda()
        
        # 1. ViTë¡œ ì´ë¯¸ì§€ íŒ¨ì¹˜ ì¶”ì¶œ
        with torch.no_grad():
            image_patches = self.backbone(processed)  # [1, 197, 768] (CLS + 196 patches)
            # CLS í† í° ì œê±°, íŒ¨ì¹˜ë§Œ ì‚¬ìš©
            if len(image_patches.shape) == 3 and image_patches.shape[1] > 1:
                image_patches = image_patches[:, 1:]  # [1, 196, 768]
        
        # 2. Query tokensë¥¼ ë°°ì¹˜ í¬ê¸°ì— ë§ê²Œ í™•ì¥
        batch_size = image_patches.shape[0]
        query_tokens = self.query_tokens.expand(batch_size, -1, -1)  # [1, 32, 768]
        
        # 3. Cross-Attention: Queryê°€ ì´ë¯¸ì§€ íŒ¨ì¹˜ë“¤ì„ ì°¸ì¡°
        attended_queries, attention_weights = self.cross_attention(
            query_tokens,    # Query: í•™ìŠµëœ ì¿¼ë¦¬ë“¤
            image_patches,   # Key: ì´ë¯¸ì§€ íŒ¨ì¹˜ë“¤  
            image_patches    # Value: ì´ë¯¸ì§€ íŒ¨ì¹˜ë“¤
        )  # [1, 32, 768]
        
        # 4. Query Transformerë¡œ ì¿¼ë¦¬ë“¤ ê°„ ìƒí˜¸ì‘ìš©
        refined_queries = self.query_transformer(attended_queries)  # [1, 32, 768]
        
        # 5. Vision-Language Projection
        vision_language_features = self.vision_language_proj(refined_queries)  # [1, 32, output_dim]
        
        return vision_language_features, attention_weights
    
    def generate_multimodal_tokens(self, x):
        """ì´ë¯¸ì§€ë¥¼ LLMì´ ì´í•´í•  ìˆ˜ ìˆëŠ” í† í°ë“¤ë¡œ ë³€í™˜ (BLIP2 ë°©ì‹)"""
        vision_tokens, attention_weights = self.forward_qformer(x)  # [1, 32, 768]
        
        # ë””ë²„ê¹…ìš© ì •ë³´
        token_strengths = torch.mean(torch.abs(vision_tokens), dim=-1)  # [1, 32]
        description = f"Vision tokens: {self.num_query_tokens} learned representations"
        
        return vision_tokens, description
    
    def forward(self, x, question_features=None):
        """ë©”ì¸ forward í•¨ìˆ˜ - Q-Former ì‚¬ìš©"""
        vision_tokens, _ = self.generate_multimodal_tokens(x)
        # í˜¸í™˜ì„±ì„ ìœ„í•´ í‰ê·  poolingìœ¼ë¡œ ë‹¨ì¼ ë²¡í„° ë°˜í™˜
        return torch.mean(vision_tokens, dim=1)  # [1, output_dim]


def load_vision_encoder(model_name: str = 'vit_base_patch16_224', **kwargs) -> VisionEncoder:
    """Vision Encoder ë¡œë”© í•¨ìˆ˜"""
    
    print(f"ğŸ–¼ï¸ Loading Vision Encoder: {model_name}")
    
    encoder = VisionEncoder(
        model_name=model_name,
        **kwargs
    )
    
    # íŒŒë¼ë¯¸í„° ê°œìˆ˜ ì¶œë ¥
    total_params = sum(p.numel() for p in encoder.parameters())
    trainable_params = sum(p.numel() for p in encoder.parameters() if p.requires_grad)
    
    print(f"ğŸ“Š Vision Encoder Stats:")
    print(f"   Total parameters: {total_params:,}")
    print(f"   Trainable parameters: {trainable_params:,}")
    print(f"   Frozen parameters: {total_params - trainable_params:,}")
    
    return encoder


 