import torch
import torch.nn as nn
import torch.nn.functional as F


class MultiModalFusion(nn.Module):
    """ë¹„ì „ê³¼ í…ìŠ¤íŠ¸ í”¼ì²˜ë¥¼ ìœµí•©í•˜ëŠ” ëª¨ë“ˆ"""
    
    def __init__(self, 
                 vision_dim=768, 
                 text_dim=768, 
                 hidden_dim=512, 
                 output_dim=768):
        super().__init__()
        
        self.vision_dim = vision_dim
        self.text_dim = text_dim
        self.hidden_dim = hidden_dim
        self.output_dim = output_dim
        
        # ê° ëª¨ë‹¬ë¦¬í‹°ë³„ projection
        self.vision_proj = nn.Linear(vision_dim, hidden_dim)
        self.text_proj = nn.Linear(text_dim, hidden_dim)
        
        # Cross-attention ë©”ì»¤ë‹ˆì¦˜
        self.cross_attention = nn.MultiheadAttention(
            embed_dim=hidden_dim,
            num_heads=8,
            dropout=0.1,
            batch_first=True
        )
        
        # ìœµí•© ë ˆì´ì–´
        self.fusion_layer = nn.Sequential(
            nn.Linear(hidden_dim * 2, hidden_dim),
            nn.ReLU(),
            nn.Dropout(0.1),
            nn.Linear(hidden_dim, output_dim),
            nn.LayerNorm(output_dim)
        )
        
    def forward(self, vision_features, text_features):
        """
        Args:
            vision_features: (B, vision_dim) - ë¹„ì „ í”¼ì²˜
            text_features: (B, text_dim) - í…ìŠ¤íŠ¸ í”¼ì²˜
            
        Returns:
            fused_features: (B, output_dim) - ìœµí•©ëœ í”¼ì²˜
        """
        batch_size = vision_features.size(0)
        
        # 1. ê° ëª¨ë‹¬ë¦¬í‹°ë¥¼ hidden_dimìœ¼ë¡œ projection
        v_proj = self.vision_proj(vision_features)  # (B, hidden_dim)
        t_proj = self.text_proj(text_features)      # (B, hidden_dim)
        
        # 2. Cross-attentionì„ ìœ„í•´ sequence ì°¨ì› ì¶”ê°€
        v_seq = v_proj.unsqueeze(1)  # (B, 1, hidden_dim)
        t_seq = t_proj.unsqueeze(1)  # (B, 1, hidden_dim)
        
        # 3. Vision â†’ Text attention
        v2t_attended, _ = self.cross_attention(v_seq, t_seq, t_seq)  # (B, 1, hidden_dim)
        
        # 4. Text â†’ Vision attention  
        t2v_attended, _ = self.cross_attention(t_seq, v_seq, v_seq)  # (B, 1, hidden_dim)
        
        # 5. Squeeze back to (B, hidden_dim)
        v2t_attended = v2t_attended.squeeze(1)
        t2v_attended = t2v_attended.squeeze(1)
        
        # 6. Concatenate and fuse
        fused_input = torch.cat([v2t_attended, t2v_attended], dim=-1)  # (B, hidden_dim*2)
        fused_features = self.fusion_layer(fused_input)  # (B, output_dim)
        
        return fused_features


class EnhancedMultiModalProcessor:
    """í–¥ìƒëœ ë©€í‹°ëª¨ë‹¬ ì²˜ë¦¬ê¸°"""
    
    def __init__(self, vision_encoder, text_encoder, language_model):
        self.vision_encoder = vision_encoder
        self.text_encoder = text_encoder  
        self.language_model = language_model
        
        # ìœµí•© ëª¨ë“ˆ
        self.fusion_model = MultiModalFusion(
            vision_dim=768,
            text_dim=768, 
            hidden_dim=512,
            output_dim=768
        )
        
        if torch.cuda.is_available():
            self.fusion_model = self.fusion_model.cuda()
            
    def process_multimodal_input(self, image_tensor, question_text, choices):
        """ë©€í‹°ëª¨ë‹¬ ì…ë ¥ ì²˜ë¦¬"""
        
        # 1. ê° ëª¨ë‹¬ë¦¬í‹°ë³„ í”¼ì²˜ ì¶”ì¶œ
        with torch.no_grad():
            vision_features = self.vision_encoder(image_tensor)  # (1, 768)
            text_features = self.text_encoder([question_text])   # (1, 768)
        
        # 2. ë©€í‹°ëª¨ë‹¬ ìœµí•©
        fused_features = self.fusion_model(vision_features, text_features)  # (1, 768)
        
        # 3. ìœµí•©ëœ í”¼ì²˜ ì •ë³´ë¥¼ í”„ë¡¬í”„íŠ¸ì— í¬í•¨
        prompt = self.create_informed_prompt(
            question_text, 
            choices, 
            vision_features, 
            text_features, 
            fused_features
        )
        
        # 4. LLMìœ¼ë¡œ ì‘ë‹µ ìƒì„±
        response = self.language_model.generate_text(prompt, max_new_tokens=10)
        
        return response, {
            'vision_features': vision_features,
            'text_features': text_features, 
            'fused_features': fused_features
        }
    
    def create_informed_prompt(self, question, choices, vision_feat, text_feat, fused_feat):
        """ìœµí•©ëœ í”¼ì²˜ ì •ë³´ë¥¼ í™œìš©í•œ í”„ë¡¬í”„íŠ¸ ìƒì„±"""
        
        # í”¼ì²˜ì˜ ì£¼ìš” íŠ¹ì„± ë¶„ì„
        vision_strength = vision_feat.norm().item()
        text_strength = text_feat.norm().item()
        fusion_strength = fused_feat.norm().item()
        
        prompt = (
            "You are analyzing an image-question pair with the following characteristics:\n"
            f"- Visual content strength: {vision_strength:.2f}\n"
            f"- Text complexity: {text_strength:.2f}\n" 
            f"- Multimodal alignment: {fusion_strength:.2f}\n\n"
            "Based on this multimodal analysis, answer the following question:\n\n"
            f"Question: {question}\n"
            "Choices:\n"
        )
        
        for i, choice in enumerate(choices):
            prompt += f"{chr(65+i)}. {choice}\n"
            
        prompt += "\nAnswer:"
        
        return prompt


def demonstrate_working_principle():
    """ì‘ë™ ì›ë¦¬ ì‹œì—°"""
    print("ğŸ”¬ MultiModal System Working Principle:")
    print("="*50)
    
    print("\n1ï¸âƒ£ ì´ë¯¸ì§€ ì²˜ë¦¬:")
    print("   ğŸ“¸ Image (224x224x3) â†’ VMamba â†’ MambaNeck â†’ 768D vector")
    
    print("\n2ï¸âƒ£ í…ìŠ¤íŠ¸ ì²˜ë¦¬:")
    print("   ğŸ“ Question text â†’ sentence-transformers â†’ 768D vector")
    
    print("\n3ï¸âƒ£ ë©€í‹°ëª¨ë‹¬ ìœµí•©:")
    print("   ğŸ”— Vision (768D) + Text (768D) â†’ Cross-attention â†’ Fused (768D)")
    
    print("\n4ï¸âƒ£ ì§€ëŠ¥í˜• í”„ë¡¬í”„íŠ¸:")
    print("   ğŸ§  Fused features â†’ Informed prompt â†’ phi-2 â†’ Answer")
    
    print("\nâœ¨ í•µì‹¬ ì°¨ì´ì :")
    print("   âŒ ê¸°ì¡´: ì´ë¯¸ì§€ ë¬´ì‹œ, í…ìŠ¤íŠ¸ë§Œ ì‚¬ìš©")
    print("   âœ… ê°œì„ : ì´ë¯¸ì§€+í…ìŠ¤íŠ¸ ìœµí•©, ë©€í‹°ëª¨ë‹¬ ì¶”ë¡ ")


if __name__ == "__main__":
    demonstrate_working_principle()
    
    # ê°„ë‹¨í•œ ìœµí•© ëª¨ë“ˆ í…ŒìŠ¤íŠ¸
    fusion = MultiModalFusion()
    
    # ë”ë¯¸ ë°ì´í„°ë¡œ í…ŒìŠ¤íŠ¸
    dummy_vision = torch.randn(2, 768)
    dummy_text = torch.randn(2, 768)
    
    fused = fusion(dummy_vision, dummy_text)
    print(f"\nğŸ§ª Test: Vision{dummy_vision.shape} + Text{dummy_text.shape} â†’ Fused{fused.shape}") 