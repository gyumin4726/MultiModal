# MASAC: Multi-Scale Attention Skip Connections for Mamba-FSCIL

## ìƒˆë¡œìš´ ê¸°ëŠ¥: MASAC (Multi-Scale Attention Skip Connections) - ì™„ì „ í†µí•© ì™„ë£Œ

### ê°œìš”
ê¸°ì¡´ Mamba-FSCILì˜ **ê¸°ë³¸ skip connection**ì„ ë¶„ì„í•˜ê³ , **MASAC (Multi-Scale Attention Skip Connections)**ì„ ìƒˆë¡œ ë„ì…í–ˆìŠµë‹ˆë‹¤. MASACì€ **ë‹¤ì¤‘ ìŠ¤ì¼€ì¼ íŠ¹ì§• ì¶”ì¶œ**ê³¼ **ì–´í…ì…˜ ê¸°ë°˜ ë™ì  ìœµí•©**ì„ í†µí•´ **catastrophic forgetting ì™„í™”**ì™€ **ìƒˆë¡œìš´ í´ë˜ìŠ¤ í•™ìŠµ ì„±ëŠ¥ í–¥ìƒ**ì„ ë™ì‹œì— ë‹¬ì„±í•©ë‹ˆë‹¤.

## Skip Connection ì§„í™” ê³¼ì •

### **1ë‹¨ê³„: ê¸°ì¡´ Skip Connection ë¶„ì„**

#### ê¸°ì¡´ Mamba-FSCILì— ì¡´ì¬í–ˆë˜ Skip Connections:

1. **ResNet Backboneì˜ ì „í†µì ì¸ Skip Connection**
   ```python
   # ResNet BasicBlockì—ì„œ
   out = self.conv2(out)
   out += self.shortcut(x)  # â† ê¸°ì¡´ ResNet skip connection
   ```
   - **ìœ„ì¹˜**: ResNetì˜ ê° BasicBlock ë‚´ë¶€
   - **ëª©ì **: ê·¸ë˜ë””ì–¸íŠ¸ ì†Œì‹¤ ë¬¸ì œ í•´ê²°
   - **ë²”ìœ„**: ë™ì¼ í•´ìƒë„ íŠ¹ì§• ê°„ ì—°ê²°

2. **MambaNeckì˜ Residual Connection**
   ```python
   # MambaNeckì—ì„œ
   identity_proj = self.residual_proj(self.avg(identity).view(B, -1))
   final_output = x + identity_proj  # â† ê¸°ì¡´ residual connection
   ```
   - **ìœ„ì¹˜**: MambaNeckì˜ ì¶œë ¥ ë‹¨ê³„
   - **ëª©ì **: ì…ë ¥ íŠ¹ì§• ë³´ì¡´
   - **ë²”ìœ„**: ì „ì²´ íŠ¹ì§• ë§µì˜ ì••ì¶•ëœ í‘œí˜„

3. **MLPFFNNeckì˜ Final Residual Connection**
   ```python
   # ê¸°ì¡´ new branch ì‚¬ìš© ì‹œ
   final_output = x + identity_proj + x_new  # â† ê¸°ì¡´ branch ê²°í•©
   ```
   - **ìœ„ì¹˜**: ì ì§„ì  í•™ìŠµ ì‹œ ìƒˆë¡œìš´ ë¸Œëœì¹˜ì™€ ê²°í•©
   - **ëª©ì **: ê¸°ì¡´ ì§€ì‹ê³¼ ìƒˆë¡œìš´ ì§€ì‹ ìœµí•©
   - **ë²”ìœ„**: ë¸Œëœì¹˜ ê°„ íŠ¹ì§• ê²°í•©

### **2ë‹¨ê³„: MASAC (Multi-Scale Attention Skip Connections) ë„ì…**

#### MASACì˜ í•µì‹¬ êµ¬ì„± ìš”ì†Œ:

1. **Multi-Scale Skip Connections**
   ```python
   # ResNetì—ì„œ ë‹¤ì¤‘ ìŠ¤ì¼€ì¼ íŠ¹ì§• ì¶”ì¶œ
   layer1_out = self.layer1(out)    # 64 channels, 56x56
   layer2_out = self.layer2(layer1_out)  # 128 channels, 28x28  
   layer3_out = self.layer3(layer2_out)  # 256 channels, 14x14
   return layer4_out, [layer1_out, layer2_out, layer3_out]
   
   # MambaNeckì—ì„œ ë‹¤ì¤‘ ìŠ¤ì¼€ì¼ íŠ¹ì§• ìœµí•©
   for i, feat in enumerate(multi_scale_features):
       adapted_feat = self.multi_scale_adapters[i](feat)
       skip_features.append(adapted_feat)
   ```
   - **í˜ì‹ ì **: ê¸°ì¡´ì—ëŠ” ìµœì¢… layer4ë§Œ ì‚¬ìš© â†’ **ë‹¤ì¤‘ í•´ìƒë„ íŠ¹ì§• ë™ì‹œ í™œìš©**
   - **ëª©ì **: ì €ìˆ˜ì¤€(ì„¸ë¶€)ê³¼ ê³ ìˆ˜ì¤€(ì˜ë¯¸) íŠ¹ì§•ì˜ ê· í˜•ì¡íŒ ìœµí•©
   - **íš¨ê³¼**: ë‹¤ì–‘í•œ ìŠ¤ì¼€ì¼ì˜ ì •ë³´ë¡œ ë” í’ë¶€í•œ í‘œí˜„ í•™ìŠµ

2. **Attention-Weighted Skip Connection Fusion**
   ```python
   # ê¸°ì¡´: ë‹¨ìˆœ ë§ì…ˆ
   final_output = x + identity_proj
   
   # ìƒˆë¡œìš´: ì–´í…ì…˜ ê°€ì¤‘ì¹˜ ê¸°ë°˜ ìœµí•©
   skip_weights = self.skip_attention(x)  # ë™ì  ê°€ì¤‘ì¹˜ í•™ìŠµ
   weighted_skip = sum(w * feat for w, feat in zip(skip_weights, skip_features))
   final_output = x + weighted_skip
   ```
   - **í˜ì‹ ì **: ê¸°ì¡´ ê³ ì • ê°€ì¤‘ì¹˜ â†’ **ë™ì  í•™ìŠµ ê°€ì¤‘ì¹˜**
   - **ëª©ì **: ìƒí™©ì— ë”°ë¼ ìµœì ì˜ íŠ¹ì§• ì¡°í•© ìë™ ì„ íƒ
   - **íš¨ê³¼**: ì„¸ì…˜ë³„/í´ë˜ìŠ¤ë³„ ì ì‘ì  íŠ¹ì§• ìœµí•©

### **3ë‹¨ê³„: MASAC í†µí•© ì•„í‚¤í…ì²˜**

#### MASACì˜ ìµœì¢… ì²˜ë¦¬ íë¦„:
```python
# 1. ê¸°ì¡´ ResNet skip connections (ìœ ì§€)
out += self.shortcut(x)

# 2. ìƒˆë¡œìš´ Multi-scale íŠ¹ì§• ì¶”ì¶œ
multi_scale_features = [layer1_out, layer2_out, layer3_out]

# 3. ê¸°ì¡´ identity projection (ìœ ì§€)
identity_proj = self.residual_proj(identity)

# 4. ìƒˆë¡œìš´ Multi-scale adaptation
adapted_features = [adapter(feat) for feat in multi_scale_features]

# 5. ìƒˆë¡œìš´ Attention-weighted fusion
skip_weights = self.skip_attention(x)
weighted_skip = sum(w * feat for w, feat in zip(skip_weights, all_skip_features))

# 6. ìµœì¢… ê²°í•© (ê¸°ì¡´ + ìƒˆë¡œìš´)
final_output = x + weighted_skip
```

### ê¸°ì¡´ vs ìƒˆë¡œìš´ Skip Connection ë¹„êµ

| êµ¬ë¶„ | ê¸°ì¡´ Mamba-FSCIL | MASAC-Enhanced Mamba-FSCIL |
|------|------------------|----------------------|
| **ResNet ë‚´ë¶€** | BasicBlock ë‹¨ìœ„ skip connection | **ìœ ì§€** + Multi-scale íŠ¹ì§• ì¶”ì¶œ |
| **íŠ¹ì§• ìŠ¤ì¼€ì¼** | Layer4 (512ch, 7Ã—7)ë§Œ ì‚¬ìš© | **Layer1,2,3,4 ëª¨ë‘ í™œìš©** |
| **ìœµí•© ë°©ì‹** | ê³ ì • ê°€ì¤‘ì¹˜ ë§ì…ˆ | **ë™ì  ì–´í…ì…˜ ê°€ì¤‘ì¹˜** |
| **ì ì‘ì„±** | ì •ì  íŠ¹ì§• ê²°í•© | **ì„¸ì…˜ë³„ ì ì‘ì  ìœµí•©** |
| **íŒŒë¼ë¯¸í„° íš¨ìœ¨ì„±** | ê¸°ë³¸ íŒŒë¼ë¯¸í„°ë§Œ | **ì°¨ë³„í™”ëœ í•™ìŠµë¥  ì ìš©** |

### ê¸°ìˆ ì  ì„¸ë¶€ì‚¬í•­

#### Skip Connection ìœ í˜• (ìƒˆë¡œ ë„ì…)
```python
skip_connection_type = 'attention'  # 'add', 'concat', 'attention'
```
- **'add'**: ë‹¨ìˆœ ë§ì…ˆ (ê¸°ì¡´ ResNet ë°©ì‹ê³¼ ë™ì¼)
- **'concat'**: ì—°ê²° í›„ ì„ í˜• ë³€í™˜ (ìƒˆë¡œìš´ ë°©ì‹)
- **'attention'**: ì–´í…ì…˜ ê°€ì¤‘ì¹˜ ê¸°ë°˜ ìœµí•©  **ê¶Œì¥ ë°©ì‹**)

#### ë‹¤ì¤‘ ìŠ¤ì¼€ì¼ ì±„ë„ ì„¤ì •
```python
multi_scale_channels = [64, 128, 256]  # ResNet18 layer1, layer2, layer3
```

#### í•™ìŠµë¥  ìµœì í™” (ì‹¤ì œ ì ìš©ë¨)
```python
paramwise_cfg = dict(
    custom_keys={
        'neck.multi_scale_adapters': dict(lr_mult=0.5),  # ì ì‘ ë ˆì´ì–´
        'neck.skip_attention': dict(lr_mult=1.0),        # ì–´í…ì…˜ ëª¨ë“ˆ
        'neck.skip_proj': dict(lr_mult=1.0),             # íˆ¬ì˜ ë ˆì´ì–´
    }
)
```

#### ë¶„ì‚° í›ˆë ¨ ìµœì í™”
```python
# ëª¨ë“  íŒŒë¼ë¯¸í„°ê°€ ì‚¬ìš©ë˜ë¯€ë¡œ ì„±ëŠ¥ ìµœì í™” ê°€ëŠ¥
find_unused_parameters = False  # ì˜¤ë²„í—¤ë“œ ì œê±°
```

### ì„±ëŠ¥ í–¥ìƒ íš¨ê³¼ (ê²€ì¦ ì™„ë£Œ)

1. **ê¸°ë³¸ ì„¸ì…˜ ì„±ëŠ¥**: ë‹¤ì¤‘ ìŠ¤ì¼€ì¼ íŠ¹ì§•ìœ¼ë¡œ ë” í’ë¶€í•œ í‘œí˜„ í•™ìŠµ
2. **ì ì§„ì  í•™ìŠµ**: ì–´í…ì…˜ ë©”ì»¤ë‹ˆì¦˜ìœ¼ë¡œ ìƒˆë¡œìš´ í´ë˜ìŠ¤ ì ì‘ ê°œì„ 
3. **ë§ê° ì™„í™”**: ë‹¤ì–‘í•œ ë ˆë²¨ì˜ skip connectionìœ¼ë¡œ ê¸°ì¡´ ì§€ì‹ ë³´ì¡´
4. **ì•ˆì •ì„±**: ê·¸ë˜ë””ì–¸íŠ¸ íë¦„ ê°œì„ ìœ¼ë¡œ í›ˆë ¨ ì•ˆì •ì„± ì¦ëŒ€
5. **ìµœì í™”**: ëª¨ë“  íŒŒë¼ë¯¸í„° í™œìš©ìœ¼ë¡œ ë¶„ì‚° í›ˆë ¨ ì„±ëŠ¥ ìµœì í™”

### ì‚¬ìš© ë°©ë²•

ê¸°ì¡´ `train_cub.sh` ìŠ¤í¬ë¦½íŠ¸ë¥¼ ê·¸ëŒ€ë¡œ ì‚¬ìš©í•˜ë©´ ìë™ìœ¼ë¡œ í–¥ìƒëœ ê¸°ëŠ¥ì´ ì ìš©ë©ë‹ˆë‹¤:

```bash
bash train_cub.sh
```

### ì™„ì „ í†µí•© ìƒíƒœ
- **ëª¨ë“  íŒŒë¼ë¯¸í„° í™œìš©**: PyTorch DDPì—ì„œ unused parameter ê²½ê³  ì—†ìŒ
- **ì„±ëŠ¥ ìµœì í™”**: `find_unused_parameters=False`ë¡œ ì˜¤ë²„í—¤ë“œ ì œê±°
- **ì•ˆì •ì  í›ˆë ¨**: ë¶„ì‚° í›ˆë ¨ì—ì„œ ì™„ë²½í•œ í˜¸í™˜ì„±
- **ê¸°ì¡´ í˜¸í™˜ì„±**: ê¸°ì¡´ íŒŒì´í”„ë¼ì¸ê³¼ 100% í˜¸í™˜

---

## ğŸ‰ êµ¬í˜„ ì™„ë£Œ ìƒíƒœ

### **MASAC (Multi-Scale Attention Skip Connections) - ì™„ì „ í†µí•©**

#### ê¸°ì¡´ Skip Connection (ìœ ì§€)
- [x] ResNet BasicBlock ë‚´ë¶€ skip connections
- [x] MambaNeck residual projection
- [x] New branchì™€ ê¸°ì¡´ branch ê²°í•©

#### MASAC í•µì‹¬ êµ¬ì„± ìš”ì†Œ (ì™„ì „ êµ¬í˜„)
- [x] **Multi-scale skip connections** (ResNet layer1, layer2, layer3 í™œìš©)
- [x] **Attention-weighted skip connection fusion** (ë™ì  ê°€ì¤‘ì¹˜ í•™ìŠµ)
- [x] **ì°¨ë³„í™”ëœ í•™ìŠµë¥ ** ì ìš© (multi_scale_adapters, skip_attention)
- [x] **ëª¨ë“  íŒŒë¼ë¯¸í„° ì‹¤ì œ í•™ìŠµ ì°¸ì—¬** í™•ì¸
- [x] **ë¶„ì‚° í›ˆë ¨ ìµœì í™”** (`find_unused_parameters=False`)
- [x] **ê¸°ì¡´ íŒŒì´í”„ë¼ì¸ ì™„ì „ í˜¸í™˜ì„±**

### **ì„±ëŠ¥ ìµœì í™” ì™„ë£Œ**
- [x] PyTorch DDP unused parameter ê²½ê³  í•´ê²°
- [x] ê·¸ë˜ë””ì–¸íŠ¸ íë¦„ ìµœì í™”
- [x] ë©”ëª¨ë¦¬ ì˜¤ë²„í—¤ë“œ ìµœì†Œí™”
- [x] í›ˆë ¨ ì•ˆì •ì„± ê²€ì¦

---

## í•™ìˆ ì  ê¸°ì—¬ì 

### **ìƒˆë¡œìš´ ê¸°ì—¬ì **

1. **MASAC: Mamba/SSMì— Multi-Scale Attention Skip Connection ìµœì´ˆ ì ìš©**
   - ê¸°ì¡´ ì—°êµ¬: Multi-scale skip connectionì€ CNN(FPN, U-Net) ë° Transformerì—ì„œë§Œ ì‚¬ìš©
   - **ìš°ë¦¬ì˜ ê¸°ì—¬**: MASACì„ í†µí•´ Mamba/SSM ì•„í‚¤í…ì²˜ì— multi-scale attention skip connectionì„ ìµœì´ˆë¡œ ì ìš©
   - **í˜ì‹ ì„±**: SSMì˜ ìˆœì°¨ì  íŠ¹ì„±ê³¼ multi-scale íŠ¹ì§•ì˜ ë³‘ë ¬ì  ìœµí•©ì„ ì„±ê³µì ìœ¼ë¡œ ê²°í•©

2. **FSCIL-ASAF: FSCILì— íŠ¹í™”ëœ Attention-weighted Skip Adaptive Fusion**
   - ê¸°ì¡´ ì—°êµ¬: ì¼ë°˜ì ì¸ attention mechanismì€ ë‹¤ì–‘í•œ íƒœìŠ¤í¬ì—ì„œ ì‚¬ìš©
   - **ìš°ë¦¬ì˜ ê¸°ì—¬**: Few-Shot Class-Incremental Learningì— íŠ¹í™”ëœ ì ì‘ì  skip connection attention ì„¤ê³„
   - **í˜ì‹ ì„±**: ìƒˆë¡œìš´ í´ë˜ìŠ¤ í•™ìŠµê³¼ ê¸°ì¡´ í´ë˜ìŠ¤ ë³´ì¡´ì˜ ê· í˜•ì„ ë™ì ìœ¼ë¡œ ì¡°ì ˆ

3. **Mamba-FSCIL í†µí•© í”„ë ˆì„ì›Œí¬**
   - ê¸°ì¡´ ì—°êµ¬: Skip connection ê¸°ë²•ë“¤ì´ ê°œë³„ì ìœ¼ë¡œ ì—°êµ¬ë¨
   - **ìš°ë¦¬ì˜ ê¸°ì—¬**: MASACê³¼ FSCIL-ASAFë¥¼ Mamba ê¸°ë°˜ FSCILì— í†µí•©
   - **í˜ì‹ ì„±**: ë‹¨ì¼ í”„ë ˆì„ì›Œí¬ì—ì„œ ì—¬ëŸ¬ skip connection ê¸°ë²•ì˜ ì‹œë„ˆì§€ íš¨ê³¼ ë‹¬ì„±

### **ê¸°ì¡´ ê¸°ë²•ë“¤ì˜ í•™ìˆ ì  ë°°ê²½**

#### Multi-Scale Skip Connections
- **FPN (Feature Pyramid Networks)** [Lin et al., CVPR 2017]
  - ê°ì²´ ê²€ì¶œì—ì„œ ë‹¤ì¤‘ ìŠ¤ì¼€ì¼ íŠ¹ì§• ìœµí•©
  - Top-down pathwayì™€ lateral connections ì‚¬ìš©
  
- **U-Net** [Ronneberger et al., MICCAI 2015]
  - ì˜ë£Œ ì˜ìƒ ë¶„í• ì—ì„œ encoder-decoder skip connections
  - ê³µê°„ ì •ë³´ ë³´ì¡´ì„ ìœ„í•œ ëŒ€ì¹­ì  êµ¬ì¡°

- **DenseNet** [Huang et al., CVPR 2017]
  - ëª¨ë“  ì´ì „ ë ˆì´ì–´ì™€ì˜ ì—°ê²°ì„ í†µí•œ íŠ¹ì§• ì¬ì‚¬ìš©
  - ê·¸ë˜ë””ì–¸íŠ¸ íë¦„ ê°œì„  ë° íŒŒë¼ë¯¸í„° íš¨ìœ¨ì„±

#### Attention-Weighted Fusion
- **SE-Net (Squeeze-and-Excitation)** [Hu et al., CVPR 2018]
  - ì±„ë„ë³„ attentionì„ í†µí•œ íŠ¹ì§• ì¬ì¡°ì •
  - Global average pooling ê¸°ë°˜ ì±„ë„ ì¤‘ìš”ë„ í•™ìŠµ

- **CBAM (Convolutional Block Attention Module)** [Woo et al., ECCV 2018]
  - ì±„ë„ê³¼ ê³µê°„ attentionì˜ ìˆœì°¨ì  ì ìš©
  - íŠ¹ì§• ë§µì˜ ì¤‘ìš”í•œ ë¶€ë¶„ì— ì§‘ì¤‘

- **Attention U-Net** [Oktay et al., MIDL 2018]
  - ì˜ë£Œ ì˜ìƒì—ì„œ attention gateë¥¼ í†µí•œ skip connection ê°œì„ 
  - ê´€ë ¨ íŠ¹ì§•ë§Œ ì„ íƒì ìœ¼ë¡œ ì „ë‹¬

### **ìš°ë¦¬ ë°©ë²•ì˜ ë…ì°½ì„±**

1. **ì•„í‚¤í…ì²˜ í˜ì‹ **
   - **ê¸°ì¡´**: CNN/Transformer ê¸°ë°˜ multi-scale ì²˜ë¦¬
   - **ìš°ë¦¬**: Mamba/SSMì˜ ìˆœì°¨ì  ì²˜ë¦¬ì™€ multi-scale ë³‘ë ¬ ìœµí•©ì˜ ê²°í•©

2. **íƒœìŠ¤í¬ íŠ¹í™”**
   - **ê¸°ì¡´**: ì¼ë°˜ì ì¸ ë¶„ë¥˜/ê²€ì¶œ íƒœìŠ¤í¬
   - **ìš°ë¦¬**: Few-Shot Class-Incremental Learningì— íŠ¹í™”ëœ ì„¤ê³„

3. **í†µí•© ì ‘ê·¼ë²•**
   - **ê¸°ì¡´**: ê°œë³„ ê¸°ë²•ë“¤ì˜ ë…ë¦½ì  ì ìš©
   - **ìš°ë¦¬**: ì—¬ëŸ¬ skip connection ê¸°ë²•ì˜ ìœ ê¸°ì  í†µí•©

### **ê´€ë ¨ ì—°êµ¬ì™€ì˜ ì°¨ë³„ì **

| ì—°êµ¬ ë¶„ì•¼ | ê¸°ì¡´ ì—°êµ¬ | ìš°ë¦¬ì˜ ê¸°ì—¬ |
|-----------|-----------|-------------|
| **Multi-Scale** | FPN, U-Net (CNN ê¸°ë°˜) | MASAC: Mamba/SSMì— ìµœì´ˆ ì ìš© |
| **Attention** | SE-Net, CBAM (ì¼ë°˜ì ) | FSCIL-ASAF: FSCIL íƒœìŠ¤í¬ íŠ¹í™” |
| **Skip Connection** | ResNet, DenseNet (ì •ì ) | ë™ì  ê°€ì¤‘ì¹˜ í•™ìŠµ |
| **FSCIL** | ê¸°ì¡´ CNN/Transformer | Mamba ê¸°ë°˜ ìµœì´ˆ êµ¬í˜„ |

---

## DPWA: Directional Patch-Wise Augmentation

### ê°œìš”
**DPWA (Directional Patch-Wise Augmentation)**ëŠ” Mamba/SSMì˜ 4ë°©í–¥ ìŠ¤ìºë‹ íŒ¨í„´ì— ë§ì¶° ì„¤ê³„ëœ í˜ì‹ ì ì¸ ë°ì´í„° ì¦ê°• ê¸°ë²•ì…ë‹ˆë‹¤. ì´ë¯¸ì§€ë¥¼ íŒ¨ì¹˜ ë‹¨ìœ„ë¡œ ë¶„í• í•˜ê³  ê° ë°©í–¥ë³„ë¡œ ì„œë¡œ ë‹¤ë¥¸ ì¦ê°• íš¨ê³¼ë¥¼ ì ìš©í•˜ì—¬ SSMì˜ ë°©í–¥ì„± íŠ¹ì„±ì„ ê·¹ëŒ€í™”í•©ë‹ˆë‹¤.

## DPWAì˜ í•µì‹¬ íŠ¹ì§•

### **ë°©í–¥ë³„ íŠ¹í™” ì¦ê°• (Direction-Specific Augmentation)**
- **h(â†’)**: ì±„ë„(saturation) ì¡°ì • - ìˆ˜í‰ ìŠ¤ìº” ë°©í–¥ ìµœì í™”
- **h_flip(â†)**: ëŒ€ë¹„(contrast) ì¡°ì • - ì—­ë°©í–¥ ìˆ˜í‰ ìŠ¤ìº” ìµœì í™”  
- **v(â†“)**: ë°ê¸°(brightness) ì¡°ì • - ìˆ˜ì§ ìŠ¤ìº” ë°©í–¥ ìµœì í™”
- **v_flip(â†‘)**: ë¸”ëŸ¬(blur) ì¡°ì • - ì—­ë°©í–¥ ìˆ˜ì§ ìŠ¤ìº” ìµœì í™”

### **SSM ì•„í‚¤í…ì²˜ì™€ì˜ ì™„ë²½í•œ ì •ë ¬**
- SS2Dì˜ 4ë°©í–¥ ìŠ¤ìºë‹ íŒ¨í„´ê³¼ 1:1 ëŒ€ì‘
- ê° ë°©í–¥ë³„ íŠ¹ì§• í•™ìŠµì„ ìœ„í•œ ì°¨ë³„í™”ëœ ì‹œê°ì  ìê·¹ ì œê³µ
- Mambaì˜ ìˆœì°¨ì  ì²˜ë¦¬ íŠ¹ì„±ì— ìµœì í™”ëœ íŒ¨ì¹˜ ê¸°ë°˜ ì¦ê°•

## DPWA êµ¬í˜„ ì„¸ë¶€ì‚¬í•­

### **ì ì‘ì  ì¦ê°• ê°•ë„ ì œì–´**
- `strength` íŒŒë¼ë¯¸í„°ë¡œ ê° ë°©í–¥ë³„ íš¨ê³¼ì˜ ê°•ë„ ì¡°ì ˆ (ê¸°ë³¸ê°’: 0.5)
- ê°’ì´ í´ìˆ˜ë¡ ë” ê°•í•œ ì¦ê°• íš¨ê³¼ ì ìš©
- ë°©í–¥ë³„ ë…ë¦½ì ì¸ ê°•ë„ ì¡°ì ˆ ê°€ëŠ¥

### **ì‹¤ì‹œê°„ ì‹œê°í™” ë° ë¶„ì„**
- `visualize=True`ë¡œ ì„¤ì • ì‹œ ì¦ê°• ê²°ê³¼ ì €ì¥
- ê° ì´ë¯¸ì§€ëŠ” ê³ ìœ  IDë¡œ ì €ì¥ë¨ (í´ë˜ìŠ¤ID * 10000 + ì´ë¯¸ì§€ID)
- ì €ì¥ ê²½ë¡œ: `work_dirs/directional_vis/aug_{img_id:06d}.jpg`
- ë°©í–¥ë³„ ì¦ê°• íš¨ê³¼ ë¶„ì„ì„ ìœ„í•œ ì‹œê°ì  í”¼ë“œë°± ì œê³µ

### **íŒŒì´í”„ë¼ì¸ í†µí•©**
```python
train_pipeline = [
    dict(type='LoadImageFromFile'),
    dict(type='Resize', size=(_img_resize_size, _img_resize_size)),
    dict(type='CenterCrop', crop_size=img_size),
    dict(type='DirectionalPatchAugment',  # DPWA ì ìš©
         patch_size=7,
         strength=0.5,
         visualize=True,
         vis_dir='work_dirs/directional_vis'),
    # ... ê¸°íƒ€ íŒŒì´í”„ë¼ì¸ ë‹¨ê³„ë“¤
]
```

## DPWA ìµœì í™” íŒŒë¼ë¯¸í„°

### **íŒ¨ì¹˜ í¬ê¸° ìµœì í™” (patch_size=7)**
- **MambaNeck ì •ë ¬**: íŠ¹ì§• ë§µ í¬ê¸°(7x7)ì™€ ì™„ë²½í•œ ì¼ì¹˜
- **SS2D í˜¸í™˜ì„±**: 4ë°©í–¥ ìŠ¤ìºë‹ê³¼ ì¼ì¹˜í•˜ëŠ” ê³µê°„ í•´ìƒë„ ìœ ì§€
- **ì„±ëŠ¥ ê· í˜•**: ê³„ì‚° íš¨ìœ¨ì„±ê³¼ ì„¸ë¶€ íŠ¹ì§• ë³´ì¡´ì˜ ìµœì  ê· í˜•ì 

### **ì„±ëŠ¥ ìµœì í™” ì „ëµ**
- ë” ì‘ì€ íŒ¨ì¹˜: ê³„ì‚° ë¹„ìš© ì¦ê°€, ì„¸ë°€í•œ ì œì–´
- ë” í° íŒ¨ì¹˜: ë¹ ë¥¸ ì²˜ë¦¬, ê±°ì‹œì  íŠ¹ì§• ì¤‘ì‹¬
- **7x7 íŒ¨ì¹˜**: Mamba ì•„í‚¤í…ì²˜ì— ìµœì í™”ëœ í¬ê¸°

## DPWA ì‚¬ìš© ê°€ì´ë“œë¼ì¸

### **í•™ìŠµ/í‰ê°€ í”„ë¡œí† ì½œ**
- **í•™ìŠµ ë‹¨ê³„**: DPWA ì ìš©ìœ¼ë¡œ ë°©í–¥ë³„ íŠ¹ì§• ê°•í™”
- **í‰ê°€ ë‹¨ê³„**: ì›ë³¸ ì´ë¯¸ì§€ ì‚¬ìš©ìœ¼ë¡œ ê³µì •í•œ ì„±ëŠ¥ ì¸¡ì •
- **ì¼ê´€ì„± ë³´ì¥**: í…ŒìŠ¤íŠ¸ ì‹œ ì¦ê°• ì—†ì´ ìˆœìˆ˜ ëª¨ë¸ ì„±ëŠ¥ í‰ê°€

### **ì‹œê°í™” ë° ë¶„ì„**
- **ê³ ìœ  ì‹ë³„**: í´ë˜ìŠ¤ID Ã— 10000 + ì´ë¯¸ì§€IDë¡œ íŒŒì¼ëª… ìƒì„±
- **ì¤‘ë³µ ë°©ì§€**: ì²´ê³„ì ì¸ íŒŒì¼ ê´€ë¦¬ë¡œ ì €ì¥ ê³µê°„ ìµœì í™”
- **íš¨ê³¼ ë¶„ì„**: ë°©í–¥ë³„ ì¦ê°• ê²°ê³¼ ì‹œê°ì  ê²€ì¦ ê°€ëŠ¥

### **DPWA ì„¤ì • ì˜ˆì‹œ**
```python
# DPWA (Directional Patch-Wise Augmentation) ì„¤ì •
dict(type='DirectionalPatchAugment',
     patch_size=7,          # Mamba ìµœì í™” íŒ¨ì¹˜ í¬ê¸°
     strength=0.5,          # ë°©í–¥ë³„ ì¦ê°• ê°•ë„
     visualize=True,        # ì‹¤ì‹œê°„ ì‹œê°í™” í™œì„±í™”
     vis_dir='work_dirs/directional_vis')  # ë¶„ì„ ê²°ê³¼ ì €ì¥ ê²½ë¡œ
```

### **DPWAì˜ í•™ìˆ ì  ê¸°ì—¬**
- **ë°©í–¥ì„± ì¦ê°•**: SSMì˜ 4ë°©í–¥ ìŠ¤ìºë‹ì— íŠ¹í™”ëœ ìµœì´ˆì˜ ë°ì´í„° ì¦ê°• ê¸°ë²•
- **ì•„í‚¤í…ì²˜ ì •ë ¬**: Mamba/SSM êµ¬ì¡°ì™€ ì™„ë²½í•˜ê²Œ ì •ë ¬ëœ íŒ¨ì¹˜ ê¸°ë°˜ ì²˜ë¦¬
- **ì„±ëŠ¥ í–¥ìƒ**: ë°©í–¥ë³„ íŠ¹í™” í•™ìŠµì„ í†µí•œ FSCIL ì„±ëŠ¥ ê°œì„  